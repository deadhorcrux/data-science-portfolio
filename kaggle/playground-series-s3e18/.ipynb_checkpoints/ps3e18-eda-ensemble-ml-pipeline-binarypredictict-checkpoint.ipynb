{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a912b66",
   "metadata": {
    "papermill": {
     "duration": 0.016973,
     "end_time": "2023-06-29T23:06:09.232823",
     "exception": false,
     "start_time": "2023-06-29T23:06:09.215850",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <p style=\"font-family:JetBrains Mono; font-weight:bold; letter-spacing: 2px; color:#1871c9; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #000966\">Libraries</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ce867f",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-06-29T23:06:09.267915Z",
     "iopub.status.busy": "2023-06-29T23:06:09.267483Z",
     "iopub.status.idle": "2023-06-29T23:06:48.377252Z",
     "shell.execute_reply": "2023-06-29T23:06:48.376289Z"
    },
    "papermill": {
     "duration": 39.130595,
     "end_time": "2023-06-29T23:06:48.380042",
     "exception": false,
     "start_time": "2023-06-29T23:06:09.249447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'category_encoders'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, MinMaxScaler, QuantileTransformer\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcategory_encoders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrdinalEncoder, CountEncoder, CatBoostEncoder, OneHotEncoder\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionTransformer, LabelEncoder \u001b[38;5;66;03m# OneHotEncoder\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompose\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ColumnTransformer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from itertools import combinations\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Import sklearn classes for model selection, cross validation, and performance evaluation\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer\n",
    "import seaborn as sns\n",
    "from category_encoders import OrdinalEncoder, CountEncoder, CatBoostEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder # OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from umap import UMAP\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import make_column_selector\n",
    "import shap\n",
    "\n",
    "# Import libraries for Hypertuning\n",
    "import optuna\n",
    "\n",
    "# Import libraries for gradient boosting\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.svm import NuSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from catboost import CatBoost, CatBoostRegressor, CatBoostClassifier\n",
    "from catboost import Pool\n",
    "\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings('ignore', '.*DataFrame is highly fragmented*')\n",
    "\n",
    "from colorama import Style, Fore\n",
    "blk = Style.BRIGHT + Fore.BLACK\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "res = Style.RESET_ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded541b1",
   "metadata": {
    "papermill": {
     "duration": 0.017734,
     "end_time": "2023-06-29T23:06:48.415184",
     "exception": false,
     "start_time": "2023-06-29T23:06:48.397450",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <p style=\"font-family:JetBrains Mono; font-weight:bold; letter-spacing: 2px; color:#1871c9; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #000966\">Data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93351925",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-29T23:06:48.451560Z",
     "iopub.status.busy": "2023-06-29T23:06:48.451166Z",
     "iopub.status.idle": "2023-06-29T23:06:48.832258Z",
     "shell.execute_reply": "2023-06-29T23:06:48.830946Z"
    },
    "papermill": {
     "duration": 0.402758,
     "end_time": "2023-06-29T23:06:48.835320",
     "exception": false,
     "start_time": "2023-06-29T23:06:48.432562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "filepath = '/kaggle/input/playground-series-s3e18'\n",
    "generated_filepath = '/kaggle/input/ec-mixed-class'\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(filepath, 'train.csv'), index_col=[0])\n",
    "df_test = pd.read_csv(os.path.join(filepath, 'test.csv'), index_col=[0])\n",
    "original = pd.read_csv(os.path.join(generated_filepath, 'mixed_desc.csv'), index_col=[0])\n",
    "# original_ecfp = pd.read_csv(os.path.join(generated_filepath, 'mixed_ecfp.csv'), index_col=[0])\n",
    "# original3_fcfp = pd.read_csv(os.path.join(generated_filepath, 'mixed_fcfp.csv'), index_col=[0])\n",
    "\n",
    "# Preprocessing for original\n",
    "df = original['EC1_EC2_EC3_EC4_EC5_EC6'].str.split('_').reset_index()\n",
    "df_expanded = pd.DataFrame(df['EC1_EC2_EC3_EC4_EC5_EC6'].tolist(), columns=[f'EC{i+1}' for i in range(len(df['EC1_EC2_EC3_EC4_EC5_EC6'].iloc[0]))])\n",
    "df_expanded['CIDs'] = df['CIDs']\n",
    "df_expanded = df_expanded[['CIDs'] + [f'EC{i+1}' for i in range(len(df['EC1_EC2_EC3_EC4_EC5_EC6'].iloc[0]))]]\n",
    "df_expanded.set_index('CIDs', inplace=True)\n",
    "original = pd.concat([original[df_test.columns], df_expanded], axis=1)\n",
    "\n",
    "# Define columns\n",
    "# num_cols = df_test.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "# cat_cols = df_test.select_dtypes(include=['object']).columns.tolist()\n",
    "target_cols = [\n",
    "    'EC1', \n",
    "    'EC2'\n",
    "]\n",
    "num_cols = [\n",
    "    'BertzCT',\n",
    "    'Chi1',\n",
    "    'Chi1n',\n",
    "    'Chi1v',\n",
    "    'Chi2n',\n",
    "    'Chi2v',\n",
    "    'Chi3v',\n",
    "    'Chi4n',\n",
    "    'EState_VSA1',\n",
    "    'EState_VSA2',\n",
    "    'ExactMolWt',\n",
    "    'FpDensityMorgan1',\n",
    "    'FpDensityMorgan2',\n",
    "    'FpDensityMorgan3',\n",
    "    'HallKierAlpha',\n",
    "    'HeavyAtomMolWt',\n",
    "    'Kappa3',\n",
    "    'MaxAbsEStateIndex',\n",
    "    'MinEStateIndex',\n",
    "    'NumHeteroatoms',\n",
    "    'PEOE_VSA10',\n",
    "    'PEOE_VSA14',\n",
    "    'PEOE_VSA6',\n",
    "    'PEOE_VSA7',\n",
    "    'PEOE_VSA8',\n",
    "    'SMR_VSA10',\n",
    "    'SMR_VSA5',\n",
    "    'SlogP_VSA3',\n",
    "    'VSA_EState9',\n",
    "    'fr_COO',\n",
    "    'fr_COO2',\n",
    "    ]\n",
    "binary_cols = [\n",
    "    'EC3',\n",
    "    'EC4',\n",
    "    'EC5',\n",
    "    'EC6',\n",
    "]\n",
    "\n",
    "df_train['is_generated'] = 1\n",
    "df_test['is_generated'] = 1\n",
    "original['is_generated'] = 0\n",
    "\n",
    "print(f\"train shape :{df_train.shape}, \", f\"test shape :{df_test.shape}\")\n",
    "print(f\"original shape :{original.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa2c7bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-29T23:06:48.872446Z",
     "iopub.status.busy": "2023-06-29T23:06:48.872035Z",
     "iopub.status.idle": "2023-06-29T23:06:50.178087Z",
     "shell.execute_reply": "2023-06-29T23:06:50.177016Z"
    },
    "papermill": {
     "duration": 1.327584,
     "end_time": "2023-06-29T23:06:50.180457",
     "exception": false,
     "start_time": "2023-06-29T23:06:48.852873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_frame_style(df, caption=\"\"):\n",
    "    \"\"\"Helper function to set dataframe presentation style.\n",
    "    \"\"\"\n",
    "    return df.style.background_gradient(cmap='Blues').set_caption(caption).set_table_styles([{\n",
    "    'selector': 'caption',\n",
    "    'props': [\n",
    "        ('color', 'Blue'),\n",
    "        ('font-size', '18px'),\n",
    "        ('font-weight','bold')\n",
    "    ]}])\n",
    "\n",
    "def check_data(data, title):\n",
    "    cols = data.columns.to_list()\n",
    "    display(set_frame_style(data[cols].head(),f'{title}: First 5 Rows Of Data'))\n",
    "    display(set_frame_style(data[cols].describe(),f'{title}: Summary Statistics'))\n",
    "    display(set_frame_style(data[cols].nunique().to_frame().rename({0:'Unique Value Count'}, axis=1).transpose(), f'{title}: Unique Value Counts In Each Column'))\n",
    "    display(set_frame_style(data[cols].isna().sum().to_frame().transpose(), f'{title}:Columns With Nan'))\n",
    "    \n",
    "check_data(df_train, 'Train data')\n",
    "print('-'*100)\n",
    "check_data(df_test, 'Test data')\n",
    "print('-'*100)\n",
    "check_data(original, 'Original data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7384d4fa",
   "metadata": {
    "papermill": {
     "duration": 0.024435,
     "end_time": "2023-06-29T23:06:50.230597",
     "exception": false,
     "start_time": "2023-06-29T23:06:50.206162",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <p style=\"font-family:JetBrains Mono; font-weight:bold; letter-spacing: 2px; color:#1871c9; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #000966\">EDA</p>\n",
    "**Contents:**\n",
    "1. Train, Test and Original data histograms\n",
    "2. Correlation of Features\n",
    "3. Hierarchical Clustering\n",
    "4. Pie and bar charts for Target column features\n",
    "7. Boxplot by Machine failure\n",
    "8. Violinplot by Machine failure\n",
    "9. Scatter plots after dimensionality reduction with PCA by Target and fr_COO, fr_COO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e429a8c",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-06-29T23:06:50.282326Z",
     "iopub.status.busy": "2023-06-29T23:06:50.281914Z",
     "iopub.status.idle": "2023-06-29T23:07:22.378473Z",
     "shell.execute_reply": "2023-06-29T23:07:22.377634Z"
    },
    "papermill": {
     "duration": 32.171967,
     "end_time": "2023-06-29T23:07:22.427710",
     "exception": false,
     "start_time": "2023-06-29T23:06:50.255743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_histograms(df_train, df_test, original, target_col, n_cols=3):\n",
    "    n_rows = (len(df_train.columns) - 1) // n_cols + 1\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(18, 4*n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, var_name in enumerate(df_train.columns.tolist()):\n",
    "        if var_name != 'is_generated':\n",
    "            ax = axes[i]\n",
    "            sns.distplot(df_train[var_name], kde=True, ax=ax, label='Train')\n",
    "            if var_name != target_col:\n",
    "                sns.distplot(df_test[var_name], kde=True, ax=ax, label='Test')\n",
    "            sns.distplot(original[var_name], kde=True, ax=ax, label='Original')\n",
    "            ax.set_title(f'{var_name} Distribution (Train vs Test)')\n",
    "            ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "        \n",
    "plot_histograms(df_train[num_cols], df_test[num_cols], original[num_cols], target_cols[0], n_cols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494700f",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-06-29T23:07:22.507445Z",
     "iopub.status.busy": "2023-06-29T23:07:22.506772Z",
     "iopub.status.idle": "2023-06-29T23:07:31.251845Z",
     "shell.execute_reply": "2023-06-29T23:07:31.250769Z"
    },
    "papermill": {
     "duration": 8.791595,
     "end_time": "2023-06-29T23:07:31.258149",
     "exception": false,
     "start_time": "2023-06-29T23:07:22.466554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_heatmap(df, title, figsize=(20, 20), fontsize=6):\n",
    "    # Create a mask for the diagonal elements\n",
    "    mask = np.zeros_like(df.astype(float).corr())\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set the colormap and figure size\n",
    "    colormap = plt.cm.RdBu_r\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Set the title and font properties\n",
    "    plt.title(f'{title} Correlation of Features', fontweight='bold', y=1.02, size=20)\n",
    "\n",
    "    # Plot the heatmap with the masked diagonal elements\n",
    "    sns.heatmap(df.astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1.0, \n",
    "                square=True, cmap=colormap, linecolor='white', annot=True, annot_kws={\"size\": fontsize, \"weight\": \"bold\"},\n",
    "                mask=mask)\n",
    "\n",
    "plot_heatmap(df_train[num_cols+binary_cols+target_cols], title='Train data')\n",
    "plot_heatmap(df_test[num_cols], title='Test data')\n",
    "plot_heatmap(original[num_cols+binary_cols+target_cols], title='original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fb9605",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-06-29T23:07:31.378267Z",
     "iopub.status.busy": "2023-06-29T23:07:31.377880Z",
     "iopub.status.idle": "2023-06-29T23:07:32.666454Z",
     "shell.execute_reply": "2023-06-29T23:07:32.665535Z"
    },
    "papermill": {
     "duration": 1.351841,
     "end_time": "2023-06-29T23:07:32.669267",
     "exception": false,
     "start_time": "2023-06-29T23:07:31.317426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "def hierarchical_clustering(data, title):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 10), dpi=120)\n",
    "    correlations = data.corr()\n",
    "    converted_corr = 1 - np.abs(correlations)\n",
    "    Z = linkage(squareform(converted_corr), 'complete')\n",
    "    \n",
    "    dn = dendrogram(Z, labels=data.columns, ax=ax, above_threshold_color='#ff0000', orientation='right')\n",
    "    hierarchy.set_link_color_palette(None)\n",
    "    plt.grid(axis='x')\n",
    "    plt.title(f'{title} Hierarchical clustering, Dendrogram', fontsize=18, fontweight='bold')\n",
    "    plt.show()\n",
    "\n",
    "hierarchical_clustering(df_train[num_cols+binary_cols], title='Train data')\n",
    "hierarchical_clustering(df_test[num_cols], title='Test data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed983b29",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-06-29T23:07:32.793438Z",
     "iopub.status.busy": "2023-06-29T23:07:32.793022Z",
     "iopub.status.idle": "2023-06-29T23:07:34.950588Z",
     "shell.execute_reply": "2023-06-29T23:07:34.949444Z"
    },
    "papermill": {
     "duration": 2.220335,
     "end_time": "2023-06-29T23:07:34.953114",
     "exception": false,
     "start_time": "2023-06-29T23:07:32.732779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_target_feature(df_train, target_col, figsize=(16,5), palette='colorblind', name='Train'):\n",
    "    df_train = df_train.fillna('Nan')\n",
    "    df_train = df_train.sort_values(target_col)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=figsize)\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    # Pie chart\n",
    "    pie_colors = sns.color_palette(palette, len(df_train[target_col].unique()))\n",
    "    ax[0].pie(\n",
    "        df_train[target_col].value_counts(),\n",
    "        shadow=True,\n",
    "        explode=[0.05] * len(df_train[target_col].unique()),\n",
    "        autopct='%1.f%%',\n",
    "        textprops={'size': 15, 'color': 'white'},\n",
    "        colors=pie_colors\n",
    "    )\n",
    "    ax[0].set_aspect('equal')  # Fix the aspect ratio to make the pie chart circular\n",
    "\n",
    "    # Bar plot\n",
    "    bar_colors = sns.color_palette(palette)\n",
    "    sns.countplot(\n",
    "        data=df_train,\n",
    "        y=target_col,\n",
    "        ax=ax[1],\n",
    "        palette=bar_colors\n",
    "    )\n",
    "    ax[1].set_xlabel('Count', fontsize=14)\n",
    "    ax[1].set_ylabel('')\n",
    "    ax[1].tick_params(labelsize=12)\n",
    "    ax[1].yaxis.set_tick_params(width=0)  # Remove tick lines for y-axis\n",
    "\n",
    "    fig.suptitle(f'{target_col} in {name} Dataset', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "plot_target_feature(df_train, target_cols[0], figsize=(16,5), palette='colorblind', name='Train data')\n",
    "plot_target_feature(original, target_cols[0], figsize=(16,5), palette='colorblind', name='Original data')\n",
    "plot_target_feature(df_train, target_cols[1], figsize=(16,5), palette='colorblind', name='Train data')\n",
    "plot_target_feature(original, target_cols[1], figsize=(16,5), palette='colorblind', name='Original data')\n",
    "plot_target_feature(df_train, 'fr_COO', figsize=(16,5), palette='colorblind', name='Train data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1824d1",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-06-29T23:07:35.088954Z",
     "iopub.status.busy": "2023-06-29T23:07:35.087913Z",
     "iopub.status.idle": "2023-06-29T23:08:05.445989Z",
     "shell.execute_reply": "2023-06-29T23:08:05.444792Z"
    },
    "papermill": {
     "duration": 30.526201,
     "end_time": "2023-06-29T23:08:05.546312",
     "exception": false,
     "start_time": "2023-06-29T23:07:35.020111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_boxplot(df, hue, drop_cols=[], n_cols=3, title=''):\n",
    "    sns.set_style('whitegrid')\n",
    "\n",
    "    cols = df.columns.drop([hue] + drop_cols)\n",
    "    n_rows = (len(cols) - 1) // n_cols + 1\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(14, 4*n_rows))\n",
    "\n",
    "    for i, var_name in enumerate(cols):\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "\n",
    "        ax = axes[row, col]\n",
    "        sns.boxplot(data=df, x=hue, y=var_name, ax=ax, showmeans=True, \n",
    "                    meanprops={\"marker\":\"s\",\"markerfacecolor\":\"white\", \"markeredgecolor\":\"blue\", \"markersize\":\"5\"})\n",
    "        ax.set_title(f'{var_name} by {hue}')\n",
    "        ax.set_xlabel('')\n",
    "\n",
    "    fig.suptitle(f'{title} Boxplot by {hue}', fontweight='bold', fontsize=14, y=1.005)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_boxplot(df_train[num_cols+target_cols], hue=target_cols[0], n_cols=4, title='Train data')\n",
    "plot_boxplot(df_train[num_cols+target_cols], hue=target_cols[1], n_cols=4, title='Train data')\n",
    "plot_boxplot(df_train[num_cols], hue='fr_COO', n_cols=3, title='Train data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104cd608",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-06-29T23:08:05.722826Z",
     "iopub.status.busy": "2023-06-29T23:08:05.722132Z",
     "iopub.status.idle": "2023-06-29T23:08:44.826957Z",
     "shell.execute_reply": "2023-06-29T23:08:44.825806Z"
    },
    "papermill": {
     "duration": 39.33388,
     "end_time": "2023-06-29T23:08:44.967414",
     "exception": false,
     "start_time": "2023-06-29T23:08:05.633534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_violinplot(df, hue, drop_cols=[], n_cols=2, title=''):\n",
    "    sns.set_style('whitegrid')\n",
    "\n",
    "    cols = df.columns.drop([hue] + drop_cols)\n",
    "    n_rows = (len(cols) - 1) // n_cols + 1\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(18, 4*n_rows))\n",
    "\n",
    "    for i, var_name in enumerate(cols):\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "\n",
    "        ax = axes[row, col]\n",
    "        sns.violinplot(data=df, x=hue, y=var_name, ax=ax, inner='quartile')\n",
    "        ax.set_title(f'{var_name} Distribution')\n",
    "\n",
    "    fig.suptitle(f'{title} Violin Plot by {hue}', fontweight='bold', fontsize=16, y=1.005)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_violinplot(df_train[num_cols+target_cols], hue=target_cols[0], n_cols=4, title='Train data')\n",
    "plot_violinplot(df_train[num_cols+target_cols], hue=target_cols[1], n_cols=4, title='Train data')\n",
    "plot_violinplot(df_train[num_cols], hue='fr_COO', n_cols=3, title='Train data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c051211",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-06-29T23:08:45.211993Z",
     "iopub.status.busy": "2023-06-29T23:08:45.210803Z",
     "iopub.status.idle": "2023-06-29T23:08:53.842028Z",
     "shell.execute_reply": "2023-06-29T23:08:53.841000Z"
    },
    "papermill": {
     "duration": 8.759757,
     "end_time": "2023-06-29T23:08:53.849703",
     "exception": false,
     "start_time": "2023-06-29T23:08:45.089946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decomp:\n",
    "    def __init__(self, n_components, method=\"pca\", scaler_method='standard'):\n",
    "        self.n_components = n_components\n",
    "        self.method = method\n",
    "        self.scaler_method = scaler_method\n",
    "        \n",
    "    def dimension_reduction(self, df):\n",
    "            \n",
    "        X_reduced = self.dimension_method(df)\n",
    "        df_comp = pd.DataFrame(X_reduced, columns=[f'{self.method.upper()}_{_}' for _ in range(self.n_components)], index=df.index)\n",
    "        return df_comp\n",
    "    \n",
    "    def dimension_method(self, df):\n",
    "        \n",
    "        X = self.scaler(df)\n",
    "        if self.method == \"pca\":\n",
    "            pca = PCA(n_components=self.n_components, random_state=0)\n",
    "            X_reduced = pca.fit_transform(X)\n",
    "            self.comp = pca\n",
    "        elif self.method == \"nmf\":\n",
    "            nmf = NMF(n_components=self.n_components, random_state=0)\n",
    "            X_reduced = nmf.fit_transform(X)\n",
    "        elif self.method == \"umap\":\n",
    "            comp = UMAP(n_components=self.n_components, random_state=0)\n",
    "            X_reduced = comp.fit_transform(X)\n",
    "        elif self.method == \"ica\":\n",
    "            comp = FastICA(n_components=self.n_components, whiten='unit-variance', random_state=0)\n",
    "            X_reduced = comp.fit_transform(X)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid method name: {method}\")\n",
    "        \n",
    "        return X_reduced\n",
    "    \n",
    "    def scaler(self, df):\n",
    "        \n",
    "        _df = df.copy()\n",
    "            \n",
    "        if self.scaler_method == \"standard\":\n",
    "            return StandardScaler().fit_transform(_df)\n",
    "        elif self.scaler_method == \"minmax\":\n",
    "            return MinMaxScaler().fit_transform(_df)\n",
    "        elif self.scaler_method == None:\n",
    "            return _df.values\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid scaler_method name\")\n",
    "        \n",
    "    def get_columns(self):\n",
    "        return [f'{self.method.upper()}_{_}' for _ in range(self.n_components)]\n",
    "    \n",
    "    def get_explained_variance_ratio(self):\n",
    "        return np.sum(self.comp.explained_variance_ratio_)\n",
    "    \n",
    "    def transform(self, df):\n",
    "        X = self.scaler(df)\n",
    "        X_reduced = self.comp.transform(X)\n",
    "        df_comp = pd.DataFrame(X_reduced, columns=[f'{self.method.upper()}_{_}' for _ in range(self.n_components)], index=df.index)\n",
    "        \n",
    "        return df_comp\n",
    "    \n",
    "    def decomp_plot(self, tmp, label, hue='genre'):\n",
    "        plt.figure(figsize = (16, 9))\n",
    "        sns.scatterplot(x = f\"{label}_0\", y = f\"{label}_1\", data=tmp, hue=hue, alpha=0.7, s=100, palette='muted');\n",
    "\n",
    "        plt.title(f'{label} on {hue}', fontsize = 20)\n",
    "        plt.xticks(fontsize = 14)\n",
    "        plt.yticks(fontsize = 10);\n",
    "        plt.xlabel(f\"{label} Component 1\", fontsize = 15)\n",
    "        plt.ylabel(f\"{label} Component 2\", fontsize = 15)\n",
    "    \n",
    "    \n",
    "data = df_train[num_cols].copy()\n",
    "method = 'pca'\n",
    "for target_col in target_cols:\n",
    "    decomp = Decomp(n_components=2, method=method, scaler_method='minmax')\n",
    "    decomp_feature = decomp.dimension_reduction(data)\n",
    "    decomp_feature = pd.concat([df_train[target_col], decomp_feature], axis=1)\n",
    "    decomp.decomp_plot(decomp_feature, method.upper(), target_col)\n",
    "    \n",
    "data = df_train[[_ for _ in num_cols if _ not in ['fr_COO', 'fr_COO2']]].copy()\n",
    "method = 'pca'\n",
    "for target_col in ['fr_COO', 'fr_COO2']:\n",
    "    decomp = Decomp(n_components=2, method=method, scaler_method='minmax')\n",
    "    decomp_feature = decomp.dimension_reduction(data)\n",
    "    decomp_feature = pd.concat([df_train[target_col], decomp_feature], axis=1)\n",
    "    decomp.decomp_plot(decomp_feature, method.upper(), target_col)    \n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d757be",
   "metadata": {
    "papermill": {
     "duration": 0.163142,
     "end_time": "2023-06-29T23:08:54.182977",
     "exception": false,
     "start_time": "2023-06-29T23:08:54.019835",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <p style=\"font-family:JetBrains Mono; font-weight:bold; letter-spacing: 2px; color:#1871c9; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #000966\">Feature Engineering</p>\n",
    "\n",
    "- `create_features()` : Create new features.\n",
    "- `add_decomp_features()` : Add NMF results as features.\n",
    "- `cat_encoder()` : select Label Encoder or Count Encoder or OneHot Encoder\n",
    "- `AggFeatureExtractor`: Create Aggregate Feature\n",
    "- **This time, instead of multi-class classification, try to solve the problem with 2-class classification, targeting EC1 and EC2 respectively.**  \n",
    "- **Creating and predicting meta-features for EC1 and EC2 will also be verified in the near term.**\n",
    "\n",
    "**Note: Not all of the above feature engineering is adapted in this kernel. Please take it as an idea.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8e2311",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-29T23:08:54.498218Z",
     "iopub.status.busy": "2023-06-29T23:08:54.497372Z",
     "iopub.status.idle": "2023-06-29T23:08:54.527216Z",
     "shell.execute_reply": "2023-06-29T23:08:54.525998Z"
    },
    "papermill": {
     "duration": 0.189644,
     "end_time": "2023-06-29T23:08:54.529837",
     "exception": false,
     "start_time": "2023-06-29T23:08:54.340193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \n",
    "    new_features = {\n",
    "        'BertzCT_MaxAbsEStateIndex_Ratio': df['BertzCT'] / (df['MaxAbsEStateIndex'] + 1e-12),\n",
    "        'BertzCT_ExactMolWt_Product': df['BertzCT'] * df['ExactMolWt'],\n",
    "        'NumHeteroatoms_FpDensityMorgan1_Ratio': df['NumHeteroatoms'] / (df['FpDensityMorgan1'] + 1e-12),\n",
    "        'VSA_EState9_EState_VSA1_Ratio': df['VSA_EState9'] / (df['EState_VSA1'] + 1e-12),\n",
    "        'PEOE_VSA10_SMR_VSA5_Ratio': df['PEOE_VSA10'] / (df['SMR_VSA5'] + 1e-12),\n",
    "        'Chi1v_ExactMolWt_Product': df['Chi1v'] * df['ExactMolWt'],\n",
    "        'Chi2v_ExactMolWt_Product': df['Chi2v'] * df['ExactMolWt'],\n",
    "        'Chi3v_ExactMolWt_Product': df['Chi3v'] * df['ExactMolWt'],\n",
    "        'EState_VSA1_NumHeteroatoms_Product': df['EState_VSA1'] * df['NumHeteroatoms'],\n",
    "        'PEOE_VSA10_Chi1_Ratio': df['PEOE_VSA10'] / (df['Chi1'] + 1e-12),\n",
    "        'MaxAbsEStateIndex_NumHeteroatoms_Ratio': df['MaxAbsEStateIndex'] / (df['NumHeteroatoms'] + 1e-12),\n",
    "        'BertzCT_Chi1_Ratio': df['BertzCT'] / (df['Chi1'] + 1e-12),\n",
    "    }\n",
    "    \n",
    "    df = df.assign(**new_features)\n",
    "    new_cols = list(new_features.keys())\n",
    "    \n",
    "    return df, new_cols\n",
    "\n",
    "def add_decomp_features(X_train, X_test, n_components=5):    \n",
    "    \n",
    "    # Select the columns\n",
    "    pca_features = X_train.select_dtypes(include=['float64']).columns.tolist()\n",
    "    n_components = len(pca_features) if n_components == 'all' else n_components\n",
    "\n",
    "    # Create the pipeline\n",
    "    pipeline = make_pipeline(MinMaxScaler(), NMF(n_components=n_components))\n",
    "    \n",
    "    # Perform\n",
    "    pipeline.fit(X_train[pca_features])\n",
    "\n",
    "    # Create column names\n",
    "    pca_columns = [f'NMF_{i}' for i in range(n_components)]\n",
    "\n",
    "    # Add PCA features to the dataframe\n",
    "    X_train[pca_columns] = pipeline.transform(X_train[pca_features])\n",
    "    X_test[pca_columns] = pipeline.transform(X_test[pca_features])\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "class MyCategoryEncoders():\n",
    "    \n",
    "    def __init__(self, cat_cols, encode='label'):\n",
    "        self.cat_cols = cat_cols\n",
    "        self.encode = encode\n",
    "        \n",
    "    def cat_encoder(self, X_train, X_test):\n",
    "        if self.encode == 'label':\n",
    "            ## Label Encoder\n",
    "            encoder = OrdinalEncoder(cols=self.cat_cols)\n",
    "            train_encoder = encoder.fit_transform(X_train[self.cat_cols]).astype(int)\n",
    "            test_encoder = encoder.transform(X_test[self.cat_cols]).astype(int)\n",
    "            X_train[self.cat_cols] = train_encoder[self.cat_cols]\n",
    "            X_test[self.cat_cols] = test_encoder[self.cat_cols]\n",
    "            encoder_cols = self.cat_cols\n",
    "        elif self.encode == 'count':\n",
    "            ## Count Encoder\n",
    "            encoder = CountEncoder(cols=self.cat_cols)\n",
    "            train_encoder = encoder.fit_transform(X_train[self.cat_cols]).astype(int).add_suffix('_count')\n",
    "            test_encoder = encoder.transform(X_test[self.cat_cols]).astype(int).add_suffix('_count')\n",
    "            X_train = pd.concat([X_train, train_encoder], axis=1)\n",
    "            X_test = pd.concat([X_test, test_encoder], axis=1)\n",
    "            encoder_cols = list(train_encoder.columns)\n",
    "        else:\n",
    "            ## OneHot Encoder\n",
    "            encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "            train_encoder = encoder.fit_transform(X_train[self.cat_cols]).astype(int)\n",
    "            test_encoder = encoder.transform(X_test[self.cat_cols]).astype(int)\n",
    "            X_train = pd.concat([X_train, train_encoder], axis=1)\n",
    "            X_test = pd.concat([X_test, test_encoder], axis=1)\n",
    "            X_train.drop(self.cat_cols, axis=1, inplace=True)\n",
    "            X_test.drop(self.cat_cols, axis=1, inplace=True)\n",
    "            encoder_cols = list(train_encoder.columns)\n",
    "\n",
    "        return X_train, X_test, encoder_cols\n",
    "\n",
    "class AggFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, group_col, agg_col, agg_func):\n",
    "        self.group_col = group_col\n",
    "        self.group_col_name = ''\n",
    "        for col in group_col:\n",
    "            self.group_col_name += col\n",
    "        self.agg_col = agg_col\n",
    "        self.agg_func = agg_func\n",
    "        self.agg_df = None\n",
    "        self.medians = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        group_col = self.group_col\n",
    "        agg_col = self.agg_col\n",
    "        agg_func = self.agg_func\n",
    "        \n",
    "        self.agg_df = X.groupby(group_col)[agg_col].agg(agg_func)\n",
    "        self.agg_df.columns = [f'{self.group_col_name}_{agg}_{_agg_col}' for _agg_col in agg_col for agg in agg_func]\n",
    "        self.medians = X[agg_col].median()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        group_col = self.group_col\n",
    "        agg_col = self.agg_col\n",
    "        agg_func = self.agg_func\n",
    "        agg_df = self.agg_df\n",
    "        medians = self.medians\n",
    "        \n",
    "        X_merged = pd.merge(X, agg_df, left_on=group_col, right_index=True, how='left')\n",
    "        X_merged.fillna(medians, inplace=True)\n",
    "        X_agg = X_merged.loc[:, [f'{self.group_col_name}_{agg}_{_agg_col}' for _agg_col in agg_col for agg in agg_func]]\n",
    "        \n",
    "        return X_agg\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        X_agg = self.transform(X)\n",
    "        return X_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c1afd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-29T23:08:54.842142Z",
     "iopub.status.busy": "2023-06-29T23:08:54.841768Z",
     "iopub.status.idle": "2023-06-29T23:08:57.922317Z",
     "shell.execute_reply": "2023-06-29T23:08:57.921374Z"
    },
    "papermill": {
     "duration": 3.239647,
     "end_time": "2023-06-29T23:08:57.924514",
     "exception": false,
     "start_time": "2023-06-29T23:08:54.684867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_trains = []\n",
    "X_trains = []\n",
    "X_tests = []\n",
    "\n",
    "# Define Categorical Features\n",
    "cat_cols = ['fr_COO'] # 'fr_COO2'\n",
    "num_cols = list(set(num_cols) - set(cat_cols))\n",
    "\n",
    "# Concatenate train and original dataframes, and prepare train and test sets\n",
    "train = pd.concat([df_train, original]).drop_duplicates()\n",
    "test = df_test.copy()\n",
    "\n",
    "for target_col in target_cols:\n",
    "    print(f'--{target_col}--')\n",
    "    \n",
    "    X_train = train.drop(binary_cols+target_cols, axis=1).reset_index(drop=True)\n",
    "    y_train = train[f'{target_col}'].reset_index(drop=True).astype(int)\n",
    "    X_test = test.reset_index(drop=True)\n",
    "    \n",
    "    # ------------------\n",
    "    # Create combination Features\n",
    "    # ------------------\n",
    "    X_train, _ = create_features(X_train)\n",
    "    X_test, _ = create_features(X_test)\n",
    "    \n",
    "    # ------------------\n",
    "    # Create Decomposion Features\n",
    "    # ------------------\n",
    "    # if target_col == 'EC2':\n",
    "    #     X_train, X_test = add_decomp_features(X_train, X_test)\n",
    "    \n",
    "    # ------------------\n",
    "    # Categorical encoder\n",
    "    # ------------------\n",
    "    # ce = MyCategoryEncoders(cat_cols, encode='count')\n",
    "    # X_train, X_test, _ = ce.cat_encoder(X_train, X_test)\n",
    "    \n",
    "    # ------------------\n",
    "    # Aggregate Features\n",
    "    # ------------------\n",
    "    group_cols = [\n",
    "        ['EState_VSA2'], ['HallKierAlpha'], ['NumHeteroatoms'], \n",
    "        ['PEOE_VSA10'], ['PEOE_VSA14'], ['PEOE_VSA6'], ['PEOE_VSA7'], ['PEOE_VSA8'],\n",
    "        ['SMR_VSA10'], ['SMR_VSA5'], ['SlogP_VSA3'], ['fr_COO'], #['fr_COO2'],\n",
    "    ]\n",
    "    agg_col = [\n",
    "        'BertzCT', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3v', 'Chi4n',\n",
    "        'EState_VSA1', 'ExactMolWt', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3',\n",
    "        'HeavyAtomMolWt', 'Kappa3', 'MaxAbsEStateIndex', 'MinEStateIndex', 'VSA_EState9'\n",
    "    ]\n",
    "    agg_train, agg_test = [], []\n",
    "    for group_col in group_cols:\n",
    "        agg_extractor = AggFeatureExtractor(group_col=group_col, agg_col=agg_col, agg_func=['mean', 'std'])\n",
    "        agg_extractor.fit(pd.concat([X_train, X_test], axis=0))\n",
    "        agg_train.append(agg_extractor.transform(X_train))\n",
    "        agg_test.append(agg_extractor.transform(X_test))\n",
    "    X_train = pd.concat([X_train] + agg_train, axis=1).fillna(0)\n",
    "    X_test = pd.concat([X_test] + agg_test, axis=1).fillna(0)\n",
    "    \n",
    "    # ------------------\n",
    "    # StandardScaler\n",
    "    # ------------------\n",
    "    # sc = StandardScaler() # MinMaxScaler or StandardScaler\n",
    "    # X_train[num_cols+new_cols1+new_cols2] = sc.fit_transform(X_train[num_cols+new_cols1+new_cols2])\n",
    "    # X_test[num_cols+new_cols1+new_cols2] = sc.transform(X_test[num_cols+new_cols1+new_cols2])\n",
    "    \n",
    "    # ------------------\n",
    "    # Drop Features\n",
    "    # Highly correlated \n",
    "    # ['ExactMolWt', 'FpDensityMorgan1', 'FpDensityMorgan2', 'fr_COO']\n",
    "    # ['FpDensityMorgan2', 'FpDensityMorgan3', 'HeavyAtomMolWt', 'fr_COO2']\n",
    "    # ------------------\n",
    "    drop_cols = ['is_generated', 'fr_COO2']\n",
    "    X_train = X_train.drop(drop_cols, axis=1) # +[drop_target_col]\n",
    "    X_test = X_test.drop(drop_cols, axis=1)\n",
    "    \n",
    "    print(f\"X_train shape :{X_train.shape} , y_train shape :{y_train.shape}\")\n",
    "    print(f\"X_test shape :{X_test.shape}\")\n",
    "    print(f\"X_train ->  isnull :{X_train.isnull().values.sum()}\", f\", isinf :{np.isinf(X_train).values.sum()}\")\n",
    "    print(f\"X_test -> isnull :{X_test.isnull().values.sum()}\", f\", isinf :{np.isinf(X_train).values.sum()}\")\n",
    "    display(X_train.head(3))\n",
    "    \n",
    "    # Store Train and Test Data\n",
    "    X_trains.append(X_train)\n",
    "    y_trains.append(y_train)\n",
    "    X_tests.append(X_test)\n",
    "\n",
    "del train, test, df_train, df_test, agg_train, agg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbbd5e3",
   "metadata": {
    "papermill": {
     "duration": 0.161903,
     "end_time": "2023-06-29T23:08:58.250798",
     "exception": false,
     "start_time": "2023-06-29T23:08:58.088895",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <p style=\"font-family:JetBrains Mono; font-weight:bold; letter-spacing: 2px; color:#1871c9; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #000966\">Data Splitting</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9126e95d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-29T23:08:58.568473Z",
     "iopub.status.busy": "2023-06-29T23:08:58.567604Z",
     "iopub.status.idle": "2023-06-29T23:08:58.580267Z",
     "shell.execute_reply": "2023-06-29T23:08:58.579456Z"
    },
    "papermill": {
     "duration": 0.175569,
     "end_time": "2023-06-29T23:08:58.582879",
     "exception": false,
     "start_time": "2023-06-29T23:08:58.407310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Splitter:\n",
    "    def __init__(self, kfold=True, n_splits=5, cat_df=pd.DataFrame(), test_size=0.5):\n",
    "        self.n_splits = n_splits\n",
    "        self.kfold = kfold\n",
    "        self.cat_df = cat_df\n",
    "        self.test_size = test_size\n",
    "\n",
    "    def split_data(self, X, y, random_state_list):\n",
    "        if self.kfold == 'skf':\n",
    "            for random_state in random_state_list:\n",
    "                kf = StratifiedKFold(n_splits=self.n_splits, random_state=random_state, shuffle=True)\n",
    "                for train_index, val_index in kf.split(X, self.cat_df):\n",
    "                    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "                    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "                    yield X_train, X_val, y_train, y_val, val_index\n",
    "        elif self.kfold:\n",
    "            for random_state in random_state_list:\n",
    "                kf = KFold(n_splits=self.n_splits, random_state=random_state, shuffle=True)\n",
    "                for train_index, val_index in kf.split(X, y):\n",
    "                    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "                    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "                    yield X_train, X_val, y_train, y_val, val_index\n",
    "        else:\n",
    "            for random_state in random_state_list:\n",
    "                X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.test_size, random_state=random_state)\n",
    "                yield X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7558c4fb",
   "metadata": {
    "papermill": {
     "duration": 0.157681,
     "end_time": "2023-06-29T23:08:58.897791",
     "exception": false,
     "start_time": "2023-06-29T23:08:58.740110",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <p style=\"font-family:JetBrains Mono; font-weight:bold; letter-spacing: 2px; color:#1871c9; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #000966\">Define Model</p>\n",
    "LightGBM, CatBoost Xgboost and HistGradientBoosting hyper parameters are determined by optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc6e5fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-29T23:08:59.224711Z",
     "iopub.status.busy": "2023-06-29T23:08:59.224233Z",
     "iopub.status.idle": "2023-06-29T23:08:59.277589Z",
     "shell.execute_reply": "2023-06-29T23:08:59.276449Z"
    },
    "papermill": {
     "duration": 0.221042,
     "end_time": "2023-06-29T23:08:59.280478",
     "exception": false,
     "start_time": "2023-06-29T23:08:59.059436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, target_col, n_estimators=100, device=\"cpu\", random_state=0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.device = device\n",
    "        self.random_state = random_state\n",
    "        self.models = self._define_model(target_col)\n",
    "        self.models_name = list(self._define_model(target_col).keys())\n",
    "        self.len_models = len(self.models)\n",
    "        \n",
    "    def _define_model(self, target_col):\n",
    "        \n",
    "        if target_col == 'EC1':\n",
    "            xgb1_params = {\n",
    "                'n_estimators': self.n_estimators,\n",
    "                'learning_rate': 0.00764887595631511,\n",
    "                'booster': 'gbtree',\n",
    "                'lambda': 0.0379675844718388,\n",
    "                'alpha': 0.0127846822376181,\n",
    "                'subsample': 0.954836600053134,\n",
    "                'colsample_bytree': 0.303279712350842,\n",
    "                'max_depth': 9,\n",
    "                'min_child_weight': 8,\n",
    "                'eta': 7.78550117314348E-07,\n",
    "                'gamma': 2.16524511622072E-06,\n",
    "                'grow_policy': 'lossguide',\n",
    "                'n_jobs': -1,\n",
    "                'objective': 'binary:logistic',\n",
    "                'eval_metric': 'auc',\n",
    "                'verbosity': 0,\n",
    "                'random_state': self.random_state,\n",
    "            }\n",
    "            xgb2_params = {\n",
    "                'n_estimators': self.n_estimators,\n",
    "                'learning_rate': 0.00610113589892878,\n",
    "                'booster': 'gbtree',\n",
    "                'lambda': 8.96906460616343E-08,\n",
    "                'alpha': 0.179699301214927,\n",
    "                'subsample': 0.693147524348261,\n",
    "                'colsample_bytree': 0.201796295242956,\n",
    "                'max_depth': 7,\n",
    "                'min_child_weight': 7,\n",
    "                'eta': 0.491486337405877,\n",
    "                'gamma': 0.0219744528297816,\n",
    "                'grow_policy': 'lossguide',\n",
    "                'n_jobs': -1,\n",
    "                'objective': 'binary:logistic',\n",
    "                'eval_metric': 'auc',\n",
    "                'verbosity': 0,\n",
    "                'random_state': self.random_state,\n",
    "            }\n",
    "        elif target_col == 'EC2':\n",
    "            xgb1_params = {\n",
    "                'n_estimators': self.n_estimators,\n",
    "                'learning_rate': 0.0258060514910791,\n",
    "                'booster': 'gbtree',\n",
    "                'lambda': 7.46721185757775E-06,\n",
    "                'alpha': 2.76013165565544E-08,\n",
    "                'subsample': 0.20132629296478,\n",
    "                'colsample_bytree': 0.45781987213833,\n",
    "                'max_depth': 5,\n",
    "                'min_child_weight': 5,\n",
    "                'eta': 3.9844926835765E-07,\n",
    "                'gamma': 0.0000620888806796158,\n",
    "                'grow_policy': 'depthwise',\n",
    "                'n_jobs': -1,\n",
    "                'objective': 'binary:logistic',\n",
    "                'eval_metric': 'auc',\n",
    "                'verbosity': 0,\n",
    "                'random_state': self.random_state,\n",
    "            }\n",
    "            xgb2_params = {\n",
    "                'n_estimators': self.n_estimators,\n",
    "                'learning_rate': 0.03045801481188,\n",
    "                'booster': 'gbtree',\n",
    "                'lambda': 0.141226751984267,\n",
    "                'alpha': 0.0000169212384166775,\n",
    "                'subsample': 0.354547691277393,\n",
    "                'colsample_bytree': 0.741230587323123,\n",
    "                'max_depth': 3,\n",
    "                'min_child_weight': 8,\n",
    "                'eta': 0.000200365560443557,\n",
    "                'gamma': 0.000793115073634548,\n",
    "                'grow_policy': 'depthwise',\n",
    "                'n_jobs': -1,\n",
    "                'objective': 'binary:logistic',\n",
    "                'eval_metric': 'auc',\n",
    "                'verbosity': 0,\n",
    "                'random_state': self.random_state,\n",
    "            }\n",
    "        if self.device == 'gpu':\n",
    "            xgb1_params['tree_method'] = 'gpu_hist'\n",
    "            xgb1_params['predictor'] = 'gpu_predictor'\n",
    "            xgb2_params['tree_method'] = 'gpu_hist'\n",
    "            xgb2_params['predictor'] = 'gpu_predictor'\n",
    "        \n",
    "        if target_col == 'EC1':\n",
    "            lgb1_params = {\n",
    "                'n_estimators': self.n_estimators,\n",
    "                'learning_rate': 0.0228011747280739,\n",
    "                'reg_alpha': 5.65344226754958E-08,\n",
    "                'reg_lambda': 0.0000457329103646115,\n",
    "                'num_leaves': 41,\n",
    "                'colsample_bytree': 0.44294848432797,\n",
    "                'subsample': 0.751451025433351,\n",
    "                'subsample_freq': 4,\n",
    "                'min_child_samples': 47,\n",
    "                'objective': 'binary',\n",
    "                'metric': 'auc', # binary_error\n",
    "                'boosting_type': 'gbdt',\n",
    "                #'is_unbalance':True,\n",
    "                # 'n_jobs': -1,\n",
    "                #'force_row_wise': True,\n",
    "                'device': self.device,\n",
    "                'random_state': self.random_state\n",
    "            }\n",
    "            lgb2_params = {\n",
    "                'n_estimators': self.n_estimators,\n",
    "                'learning_rate': 0.0150317388979459,\n",
    "                'reg_alpha': 0.000137387220669803,\n",
    "                'reg_lambda': 0.0465956586731766,\n",
    "                'num_leaves': 122,\n",
    "                'colsample_bytree': 0.577212162074888,\n",
    "                'subsample': 0.414797294649733,\n",
    "                'subsample_freq': 2,\n",
    "                'min_child_samples': 89,\n",
    "                'objective': 'binary',\n",
    "                'metric': 'auc', # binary_error\n",
    "                'boosting_type': 'gbdt',\n",
    "                #'is_unbalance':True,\n",
    "                # 'n_jobs': -1,\n",
    "                #'force_row_wise': True,\n",
    "                'device': self.device,\n",
    "                'random_state': self.random_state\n",
    "            }\n",
    "        elif target_col == 'EC2':\n",
    "            lgb1_params = {\n",
    "                'n_estimators': self.n_estimators,\n",
    "                'learning_rate': 0.00235571575065482,\n",
    "                'reg_alpha': 0.0000155412959959893,\n",
    "                'reg_lambda': 0.0000347193406478947,\n",
    "                'num_leaves': 131,\n",
    "                'colsample_bytree': 0.468735996560667,\n",
    "                'subsample': 0.428043349939151,\n",
    "                'subsample_freq': 7,\n",
    "                'min_child_samples': 14,\n",
    "                'objective': 'binary',\n",
    "                'metric': 'auc', # binary_error\n",
    "                'boosting_type': 'gbdt',\n",
    "                #'is_unbalance':True,\n",
    "                # 'n_jobs': -1,\n",
    "                #'force_row_wise': True,\n",
    "                'device': self.device,\n",
    "                'random_state': self.random_state\n",
    "            }\n",
    "            lgb2_params = {\n",
    "                'n_estimators': self.n_estimators,\n",
    "                'learning_rate': 0.0342188703436379,\n",
    "                'reg_alpha': 2.39175347967819,\n",
    "                'reg_lambda': 0.139145028543823,\n",
    "                'num_leaves': 22,\n",
    "                'colsample_bytree': 0.607700187719924,\n",
    "                'subsample': 0.608258988150217,\n",
    "                'subsample_freq': 2,\n",
    "                'min_child_samples': 35,\n",
    "                'objective': 'binary',\n",
    "                'metric': 'auc', # binary_error\n",
    "                'boosting_type': 'gbdt',\n",
    "                #'is_unbalance':True,\n",
    "                # 'n_jobs': -1,\n",
    "                #'force_row_wise': True,\n",
    "                'device': self.device,\n",
    "                'random_state': self.random_state\n",
    "            }\n",
    "        if target_col == 'EC1':\n",
    "            cat1_params = {\n",
    "                'iterations': self.n_estimators,\n",
    "                'depth': 3,\n",
    "                'learning_rate': 0.020258010893459,\n",
    "                'l2_leaf_reg': 0.583685138705941,\n",
    "                'random_strength': 0.177768021213223,\n",
    "                'od_type': \"Iter\", \n",
    "                'od_wait': 116,\n",
    "                'bootstrap_type': \"Bayesian\",\n",
    "                'grow_policy': 'Depthwise',\n",
    "                'bagging_temperature': 0.478048798393903,\n",
    "                'eval_metric': 'AUC', # AUC\n",
    "                'loss_function': 'Logloss',\n",
    "                #'auto_class_weights': 'Balanced',\n",
    "                'task_type': self.device.upper(),\n",
    "                'verbose': False,\n",
    "                'allow_writing_files': False,\n",
    "                'random_state': self.random_state\n",
    "            }\n",
    "            cat2_params = {\n",
    "                'iterations': self.n_estimators,\n",
    "                'depth': 4,\n",
    "                'learning_rate': 0.0533074594005429,\n",
    "                'l2_leaf_reg': 4.33121673696473,\n",
    "                'random_strength': 0.00420305570017096,\n",
    "                'od_type': \"IncToDec\", \n",
    "                'od_wait': 41,\n",
    "                'bootstrap_type': \"Bayesian\",\n",
    "                'grow_policy': 'Lossguide',\n",
    "                'bagging_temperature': 9.20357081888618,\n",
    "                'eval_metric': 'AUC', # AUC\n",
    "                'loss_function': 'Logloss',\n",
    "                #'auto_class_weights': 'Balanced',\n",
    "                'task_type': self.device.upper(),\n",
    "                'verbose': False,\n",
    "                'allow_writing_files': False,\n",
    "                'random_state': self.random_state\n",
    "            }\n",
    "        elif target_col == 'EC2':\n",
    "            cat1_params = {\n",
    "                'iterations': self.n_estimators,\n",
    "                'depth': 4,\n",
    "                'learning_rate': 0.0533074594005429,\n",
    "                'l2_leaf_reg': 4.33121673696473,\n",
    "                'random_strength': 0.00420305570017096,\n",
    "                'od_type': \"IncToDec\", \n",
    "                'od_wait': 41,\n",
    "                'bootstrap_type': \"Bayesian\",\n",
    "                'grow_policy': 'Lossguide',\n",
    "                'bagging_temperature': 9.20357081888618,\n",
    "                'eval_metric': 'AUC', # AUC\n",
    "                'loss_function': 'Logloss',\n",
    "                #'auto_class_weights': 'Balanced',\n",
    "                'task_type': self.device.upper(),\n",
    "                'verbose': False,\n",
    "                'allow_writing_files': False,\n",
    "                'random_state': self.random_state\n",
    "            }\n",
    "            cat2_params = {\n",
    "                'iterations': self.n_estimators,\n",
    "                'depth': 5,\n",
    "                'learning_rate': 0.0991887029880458,\n",
    "                'l2_leaf_reg': 3.42589726767107,\n",
    "                'random_strength': 0.143663354460193,\n",
    "                'od_type': \"Iter\", \n",
    "                'od_wait': 134,\n",
    "                'bootstrap_type': \"Bayesian\",\n",
    "                'grow_policy': 'Lossguide',\n",
    "                'bagging_temperature': 9.03599121747771,\n",
    "                'eval_metric': 'AUC', # AUC\n",
    "                'loss_function': 'Logloss',\n",
    "                #'auto_class_weights': 'Balanced',\n",
    "                'task_type': self.device.upper(),\n",
    "                'verbose': False,\n",
    "                'allow_writing_files': False,\n",
    "                'random_state': self.random_state\n",
    "            }\n",
    "        \n",
    "        if target_col == 'EC1':\n",
    "            hist_params = {\n",
    "                'l2_regularization': 0.654926989031482,\n",
    "                'learning_rate': 0.0366207257406611,\n",
    "                'max_iter': self.n_estimators,\n",
    "                'max_depth': 30,\n",
    "                'max_bins': 255,\n",
    "                'min_samples_leaf': 52,\n",
    "                'max_leaf_nodes':12,\n",
    "                'early_stopping': True,\n",
    "                'n_iter_no_change': 50,\n",
    "                'categorical_features': cat_cols,\n",
    "                #'class_weight':'balanced',\n",
    "                'random_state': self.random_state\n",
    "            }\n",
    "        elif target_col == 'EC2':\n",
    "            hist_params = {\n",
    "                'l2_regularization': 0.0769190915635041,\n",
    "                'learning_rate': 0.0560046631920058,\n",
    "                'max_iter': self.n_estimators,\n",
    "                'max_depth': 30,\n",
    "                'max_bins': 255,\n",
    "                'min_samples_leaf': 30,\n",
    "                'max_leaf_nodes':10,\n",
    "                'early_stopping': True,\n",
    "                'n_iter_no_change': 50,\n",
    "                'categorical_features': cat_cols,\n",
    "                #'class_weight':'balanced',\n",
    "                'random_state': self.random_state\n",
    "            }\n",
    "                    \n",
    "        mlp_params = {\n",
    "            'max_iter': 800,\n",
    "            'early_stopping': True,\n",
    "            'n_iter_no_change': 50,\n",
    "            'random_state': self.random_state,\n",
    "        }\n",
    "        \n",
    "        if target_col == 'EC1':\n",
    "            models = {\n",
    "                \"xgb\": xgb.XGBClassifier(**xgb1_params),\n",
    "                \"lgb\": lgb.LGBMClassifier(**lgb1_params),\n",
    "                \"cat\": CatBoostClassifier(**cat1_params),\n",
    "#                 \"xgb2\": xgb.XGBClassifier(**xgb2_params),\n",
    "                \"lgb2\": lgb.LGBMClassifier(**lgb2_params),\n",
    "#                 \"cat2\": CatBoostClassifier(**cat2_params),\n",
    "                'hgb': HistGradientBoostingClassifier(**hist_params),\n",
    "                'rfc': RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=self.random_state),\n",
    "                'lrc': make_pipeline(StandardScaler(), LogisticRegression(max_iter=500, random_state=self.random_state)),\n",
    "#                 'etc': ExtraTreesClassifier(n_estimators=800, n_jobs=-1, random_state=self.random_state),\n",
    "                'mlp': make_pipeline(StandardScaler(), MLPClassifier(**mlp_params, hidden_layer_sizes=(100,))),\n",
    "                'ada': AdaBoostClassifier(n_estimators=100, random_state=self.random_state)\n",
    "                #'knn': KNeighborsClassifier(n_neighbors=15, n_jobs=-1),\n",
    "                #'gbc': GradientBoostingClassifier(n_estimators=500, random_state=self.random_state)\n",
    "                #'svc': SVC(max_iter=-1, kernel=\"rbf\", gamma=\"auto\", probability=True, random_state=self.random_state),\n",
    "            }\n",
    "        elif target_col == 'EC2':\n",
    "            models = {\n",
    "                \"xgb\": xgb.XGBClassifier(**xgb1_params),\n",
    "                \"lgb\": lgb.LGBMClassifier(**lgb1_params),\n",
    "                \"cat\": CatBoostClassifier(**cat1_params),\n",
    "                \"xgb2\": xgb.XGBClassifier(**xgb2_params),\n",
    "                \"lgb2\": lgb.LGBMClassifier(**lgb2_params),\n",
    "                \"cat2\": CatBoostClassifier(**cat2_params),\n",
    "                'hgb': HistGradientBoostingClassifier(**hist_params),\n",
    "                'rfc': RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=self.random_state),\n",
    "                'lrc': make_pipeline(StandardScaler(), LogisticRegression(max_iter=500, random_state=self.random_state)),\n",
    "                'etc': ExtraTreesClassifier(n_estimators=800, n_jobs=-1, random_state=self.random_state),\n",
    "                'mlp': make_pipeline(StandardScaler(), MLPClassifier(**mlp_params, hidden_layer_sizes=(100,))),\n",
    "                'ada': AdaBoostClassifier(n_estimators=100, random_state=self.random_state)\n",
    "                #'knn': KNeighborsClassifier(n_neighbors=15, n_jobs=-1),\n",
    "                #'gbc': GradientBoostingClassifier(n_estimators=500, random_state=self.random_state)\n",
    "                #'svc': SVC(max_iter=-1, kernel=\"rbf\", gamma=\"auto\", probability=True, random_state=self.random_state),\n",
    "            }\n",
    "        \n",
    "        return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3ab4d6",
   "metadata": {
    "papermill": {
     "duration": 0.159268,
     "end_time": "2023-06-29T23:08:59.600297",
     "exception": false,
     "start_time": "2023-06-29T23:08:59.441029",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <p style=\"font-family:JetBrains Mono; font-weight:bold; letter-spacing: 2px; color:#1871c9; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #000966\">Feature Selection (RFE-CV)</p>\n",
    "RFECV is a technique for automated feature selection that combines recursive feature elimination and cross-validation to identify the optimal subset of features for a given machine learning task.\n",
    "\n",
    "**Note: RFE-CV takes a lot of time. Here n_estimators are reduced to save time. When originally used, it is recommended to run with the actual hyperparameters.**\n",
    "\n",
    "![](https://www.researchgate.net/publication/362548828/figure/fig4/AS:11431281087244666@1664503792971/Recursive-Feature-Elimination-with-Cross-ValidationRFECV-flowchart-model-diagram.png)\n",
    "\n",
    "Reference: Feature fusion based machine learning pipeline to improve breast cancer prediction, 81, p.37627–37655 (2022) https://doi.org/10.1007/s11042-022-13498-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5adb45c",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-06-29T23:08:59.920731Z",
     "iopub.status.busy": "2023-06-29T23:08:59.919763Z",
     "iopub.status.idle": "2023-06-29T23:08:59.927552Z",
     "shell.execute_reply": "2023-06-29T23:08:59.926783Z"
    },
    "papermill": {
     "duration": 0.171899,
     "end_time": "2023-06-29T23:08:59.929782",
     "exception": false,
     "start_time": "2023-06-29T23:08:59.757883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_recursive_feature_elimination(elimination, scoring, min_features_to_select, name):\n",
    "    n_scores = len(elimination.cv_results_[\"mean_test_score\"])\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(f\"{scoring}\")\n",
    "\n",
    "    # Plot the mean test scores with error bars\n",
    "    plt.errorbar(\n",
    "        range(min_features_to_select, n_scores + min_features_to_select),\n",
    "        elimination.cv_results_[\"mean_test_score\"],\n",
    "        yerr=elimination.cv_results_[\"std_test_score\"],\n",
    "        fmt='o-',\n",
    "        capsize=3,\n",
    "        markersize=4,\n",
    "    )\n",
    "\n",
    "    plt.title(f\"{name} Recursive Feature Elimination with correlated features\", fontweight='bold')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a5b8ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-29T23:09:00.252466Z",
     "iopub.status.busy": "2023-06-29T23:09:00.251750Z",
     "iopub.status.idle": "2023-06-29T23:16:09.187236Z",
     "shell.execute_reply": "2023-06-29T23:16:09.186256Z"
    },
    "papermill": {
     "duration": 429.100336,
     "end_time": "2023-06-29T23:16:09.189306",
     "exception": false,
     "start_time": "2023-06-29T23:09:00.088970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n_estimators = 50\n",
    "scoring = 'roc_auc'\n",
    "\n",
    "for target_col, X_train, y_train, X_test in zip(target_cols, X_trains, y_trains, X_tests):\n",
    "    min_features_to_select = len(X_train.columns) - 5\n",
    "    \n",
    "    print(f'--{target_col}--')\n",
    "    # drop_target_col = 'EC2' if target_col == 'EC1' else 'EC1'\n",
    "    # X_train = X_train.drop(drop_target_col, axis=1)\n",
    "    \n",
    "    splitter = Splitter(kfold=False, test_size=0.7)\n",
    "    X_train_, X_val, y_train_, y_val = next(iter(splitter.split_data(X_train, y_train, random_state_list=[42])))\n",
    "    \n",
    "    classifier = Classifier(target_col, n_estimators, device='cpu', random_state=0)\n",
    "    models = classifier.models\n",
    "\n",
    "    models_name = [_ for _ in classifier.models_name if ('xgb' in _) or ('lgb' in _) or ('cat' in _)]\n",
    "    trained_models = dict(zip(models_name, ['' for _ in range(classifier.len_models)]))\n",
    "    unnecessary_features = dict(zip(models_name, [[] for _ in range(classifier.len_models)]))\n",
    "    for name, model in models.items():\n",
    "        if ('xgb' in name) or ('lgb' in name) or ('cat' in name):\n",
    "            elimination = RFECV(\n",
    "                model, \n",
    "                step=1,\n",
    "                min_features_to_select=min_features_to_select,\n",
    "                cv=3,\n",
    "                scoring=scoring, \n",
    "                n_jobs=-1)\n",
    "            elimination.fit(X_train_, y_train_)\n",
    "            unnecessary_feature = list(X_train.columns[~elimination.get_support()])\n",
    "            idx = np.argmax(elimination.cv_results_['mean_test_score'])\n",
    "            mean_score = elimination.cv_results_['mean_test_score'][idx]\n",
    "            std_score = elimination.cv_results_['std_test_score'][idx]\n",
    "            print(f'{blu}{name}{res} {red} Best Mean{res} {scoring} {red}{mean_score:.5f} ± {std_score:.5f}{res} | N_STEP {idx}')\n",
    "            print(f\"Best unnecessary_feature: {unnecessary_feature}\")\n",
    "            removed_features = [f for i, f in enumerate(X_train.columns) if elimination.support_[i] == False]\n",
    "            ranked_features = sorted(zip(X_train.columns, elimination.ranking_), key=lambda x: x[1])\n",
    "            removed_features_by_ranking = [f[0] for f in ranked_features if f[0] in removed_features][::-1]\n",
    "            print(\"Removed features:\", removed_features_by_ranking)\n",
    "            print(f'{\"-\" * 60}')\n",
    "\n",
    "            trained_models[f'{name}'] = deepcopy(elimination)\n",
    "            unnecessary_features[f'{name}'].extend(unnecessary_feature)\n",
    "\n",
    "    unnecessary_features = np.concatenate([_ for _ in unnecessary_features.values()])\n",
    "    features = np.unique(unnecessary_features, return_counts=True)[0]\n",
    "    counts = np.unique(unnecessary_features, return_counts=True)[1]\n",
    "    drop_features = list(features[counts >= 2])\n",
    "    print(\"Features recommended to be removed:\", drop_features)\n",
    "    \n",
    "    for name, elimination in trained_models.items():\n",
    "        plot_recursive_feature_elimination(elimination, scoring, min_features_to_select, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9859c9fb",
   "metadata": {
    "papermill": {
     "duration": 0.169766,
     "end_time": "2023-06-29T23:16:09.543943",
     "exception": false,
     "start_time": "2023-06-29T23:16:09.374177",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <p style=\"font-family:JetBrains Mono; font-weight:bold; letter-spacing: 2px; color:#1871c9; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #000966\">Configuration</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4302a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-29T23:16:09.872979Z",
     "iopub.status.busy": "2023-06-29T23:16:09.871859Z",
     "iopub.status.idle": "2023-06-29T23:16:09.883056Z",
     "shell.execute_reply": "2023-06-29T23:16:09.881790Z"
    },
    "papermill": {
     "duration": 0.179029,
     "end_time": "2023-06-29T23:16:09.885371",
     "exception": false,
     "start_time": "2023-06-29T23:16:09.706342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "kfold = 'skf'\n",
    "n_splits = 5 # 10\n",
    "n_reapts = 3 # 1\n",
    "random_state = 42\n",
    "n_estimators = 9999 # 9999\n",
    "early_stopping_rounds = 200\n",
    "n_trials = 1000 # 3000\n",
    "verbose = False\n",
    "device = 'cpu'\n",
    "\n",
    "# Fix seed\n",
    "random.seed(random_state)\n",
    "random_state_list = random.sample(range(9999), n_reapts)\n",
    "# random_state_list = [42]\n",
    "# n_reapts = len(random_state_list)\n",
    "\n",
    "# metrics\n",
    "def auc(y_true, y_pred):\n",
    "    return roc_auc_score(y_true, y_pred)\n",
    "metric = auc\n",
    "metric_name = metric.__name__.upper()\n",
    "\n",
    "# To calculate runtime\n",
    "def sec_to_minsec(t):\n",
    "    min_ = int(t / 60)\n",
    "    sec = int(t - min_*60)\n",
    "    return min_, sec\n",
    "\n",
    "# Process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad28009",
   "metadata": {
    "papermill": {
     "duration": 0.162364,
     "end_time": "2023-06-29T23:16:10.211203",
     "exception": false,
     "start_time": "2023-06-29T23:16:10.048839",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <p style=\"font-family:JetBrains Mono; font-weight:bold; letter-spacing: 2px; color:#1871c9; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #000966\">One Model Xgboost</p>\n",
    "\n",
    "The xgboost model architecture is based on the following: \n",
    "[[PS S3E14, 2023] EDA and Submission](https://www.kaggle.com/code/sergiosaharovskiy/ps-s3e14-2023-eda-and-submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05e6484",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-06-29T23:16:10.539324Z",
     "iopub.status.busy": "2023-06-29T23:16:10.538887Z",
     "iopub.status.idle": "2023-06-29T23:16:10.564079Z",
     "shell.execute_reply": "2023-06-29T23:16:10.563006Z"
    },
    "papermill": {
     "duration": 0.192097,
     "end_time": "2023-06-29T23:16:10.566231",
     "exception": false,
     "start_time": "2023-06-29T23:16:10.374134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_training_process(lossfunc_key, eval_results_, best_iters_, early_stopping_rounds):\n",
    "\n",
    "    metric_score_folds = pd.DataFrame.from_dict(eval_results_).T\n",
    "    fit_rmsle = metric_score_folds.fit.apply(lambda x: x[lossfunc_key])\n",
    "    val_rmsle = metric_score_folds.val.apply(lambda x: x[lossfunc_key])\n",
    "\n",
    "    n_splits = len(metric_score_folds)\n",
    "    n_rows = math.ceil(n_splits / 3)\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, 3, figsize=(20, n_rows * 4), dpi=150)\n",
    "    ax = axes.flatten()\n",
    "\n",
    "    for i, (f, v, best_iter) in enumerate(zip(fit_rmsle, val_rmsle, best_iters_)): \n",
    "        sns.lineplot(f, color='#B90000', ax=ax[i], label='fit')\n",
    "        sns.lineplot(v, color='#048BA8', ax=ax[i], label='val')\n",
    "        ax[i].legend()\n",
    "        ax[i].spines['top'].set_visible(False)\n",
    "        ax[i].spines['right'].set_visible(False)\n",
    "        ax[i].set_title(f'Fold {i}', fontdict={'fontweight': 'bold'})\n",
    "\n",
    "        color = ['#048BA8', '#90A6B1']\n",
    "        span_range = [[0, best_iter], [best_iter + 10, best_iter + early_stopping_rounds]]\n",
    "\n",
    "        for idx, sub_title in enumerate([f'Best\\nIteration: {best_iter}', f'Early\\n Stopping: {early_stopping_rounds}']):\n",
    "            ax[i].annotate(sub_title,\n",
    "                           xy=(sum(span_range[idx]) / 2, 4000),\n",
    "                           xytext=(0, 0),\n",
    "                           textcoords='offset points',\n",
    "                           va=\"center\",\n",
    "                           ha=\"center\",\n",
    "                           color=\"w\",\n",
    "                           fontsize=12,\n",
    "                           fontweight='bold',\n",
    "                           bbox=dict(boxstyle='round4', pad=0.4, color=color[idx], alpha=0.6))\n",
    "            ax[i].axvspan(span_range[idx][0] - 0.4, span_range[idx][1] + 0.4, color=color[idx], alpha=0.07)\n",
    "\n",
    "        ax[i].set_xlim(0, best_iter + 20 + early_stopping_rounds)\n",
    "        ax[i].set_xlabel('Boosting Round', fontsize=12)\n",
    "        ax[i].set_ylabel(f'{lossfunc_key}', fontsize=12)\n",
    "        ax[i].legend(loc='upper right', title=lossfunc_key)\n",
    "\n",
    "    for j in range(i+1, n_rows * 3):\n",
    "        ax[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_feature_importance(fi):\n",
    "    fi_gain = fi[[col for col in fi.columns if col.startswith('gain')]].mean(axis=1)\n",
    "    fi_splt = fi[[col for col in fi.columns if col.startswith('split')]].mean(axis=1)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(18, 6), dpi=150)\n",
    "\n",
    "    # Split fi.\n",
    "    data_splt = fi_splt.sort_values(ascending=False)\n",
    "    data_splt = data_splt.head(20)\n",
    "    sns.barplot(x=data_splt.values, y=data_splt.index,\n",
    "                color='#1E90FF', linewidth=0.5, edgecolor=\"black\", ax=ax[0])\n",
    "    ax[0].set_title(f'Feature Importance \"Split\"', fontdict={'fontweight': 'bold'})\n",
    "    ax[0].set_xlabel(\"Importance\", fontsize=12)\n",
    "    ax[0].spines['right'].set_visible(False)\n",
    "    ax[0].spines['top'].set_visible(False)\n",
    "\n",
    "    # Gain fi.\n",
    "    data_gain = fi_gain.sort_values(ascending=False)\n",
    "    data_gain = data_gain.head(20)\n",
    "    sns.barplot(x=data_gain.values, y=data_gain.index,\n",
    "                color='#4169E1', linewidth=0.5, edgecolor=\"black\", ax=ax[1])\n",
    "    ax[1].set_title(f'Feature Importance \"Gain\"', fontdict={'fontweight': 'bold'})\n",
    "    ax[1].set_xlabel(\"Importance\", fontsize=12)\n",
    "    ax[1].spines['right'].set_visible(False)\n",
    "    ax[1].spines['top'].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b4fa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-29T23:16:10.895843Z",
     "iopub.status.busy": "2023-06-29T23:16:10.895018Z",
     "iopub.status.idle": "2023-06-29T23:23:43.130474Z",
     "shell.execute_reply": "2023-06-29T23:23:43.129257Z"
    },
    "papermill": {
     "duration": 452.591537,
     "end_time": "2023-06-29T23:23:43.321415",
     "exception": false,
     "start_time": "2023-06-29T23:16:10.729878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_estimators_ = 600\n",
    "\n",
    "test_preds_list = []\n",
    "val_scores = []\n",
    "for target_col, X_train, y_train, X_test in zip(target_cols, X_trains, y_trains, X_tests):\n",
    "    print(f'--{target_col}--')\n",
    "    # drop_target_col = 'EC2' if target_col == 'EC1' else 'EC1'\n",
    "    # X_train = X_train.drop(drop_target_col, axis=1)\n",
    "\n",
    "    feature_importances_ = pd.DataFrame(index=X_train.columns)\n",
    "    eval_results_ = {}\n",
    "    best_iters_ = []\n",
    "    oof = np.zeros((X_train.shape[0]))\n",
    "    test_preds = np.zeros((X_test.shape[0]))\n",
    "\n",
    "    splitter = Splitter(kfold=kfold, n_splits=n_splits, cat_df=y_train)\n",
    "    for i, (X_train_, X_val, y_train_, y_val, val_index) in enumerate(splitter.split_data(X_train, y_train, random_state_list=[0])):\n",
    "        fold = i % n_splits\n",
    "        m = i // n_splits\n",
    "\n",
    "        # XGB .train() requires xgboost.DMatrix.\n",
    "        # https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.DMatrix\n",
    "        fit_set = xgb.DMatrix(X_train_, y_train_)\n",
    "        val_set = xgb.DMatrix(X_val, y_val)\n",
    "        watchlist = [(fit_set, 'fit'), (val_set, 'val')]\n",
    "\n",
    "        # Training.\n",
    "        # https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.training\n",
    "        classifier = Classifier(target_col, n_estimators_, device)\n",
    "        xgb_params = classifier.models['xgb'].get_params()\n",
    "        # xgb_params = xgb.XGBClassifier(n_estimators=3000, learning_rate=0.01).get_params()\n",
    "\n",
    "        eval_results_[fold] = {}\n",
    "        model = xgb.train(\n",
    "            num_boost_round=xgb_params['n_estimators'],\n",
    "            params=xgb_params,\n",
    "            dtrain=fit_set,\n",
    "            evals=watchlist,\n",
    "            evals_result=eval_results_[fold],\n",
    "            verbose_eval=False,\n",
    "            callbacks=[xgb.callback.EarlyStopping(early_stopping_rounds, data_name='val', save_best=True)])\n",
    "\n",
    "        val_preds = model.predict(val_set)\n",
    "        test_preds += model.predict(xgb.DMatrix(X_test)) / n_splits\n",
    "\n",
    "        oof[val_index] = val_preds\n",
    "\n",
    "        val_score = metric(y_val, val_preds)\n",
    "        best_iter = model.best_iteration\n",
    "        best_iters_.append(best_iter)\n",
    "        val_scores.append(val_score)\n",
    "        print(f'Fold: {blu}{fold:>3}{res}| {metric_name}: {blu}{val_score:.5f}{res}' f' | Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "        # Stores the feature importances\n",
    "        feature_importances_[f'gain_{fold}'] = feature_importances_.index.map(model.get_score(importance_type='gain'))\n",
    "        feature_importances_[f'split_{fold}'] = feature_importances_.index.map(model.get_score(importance_type='weight'))\n",
    "\n",
    "    # Submission\n",
    "    sub = pd.read_csv(os.path.join(filepath, 'sample_submission.csv'))\n",
    "    sub[f'{target_col}'] = test_preds\n",
    "    sub.to_csv(f'xgb_{target_col}_submission.csv', index=False)\n",
    "    test_preds_list.append(test_preds) # np.round(test_preds)\n",
    "    # xgb_test_preds = test_preds.copy()\n",
    "\n",
    "    mean_cv_score_full = metric(y_train, oof)\n",
    "    print(f'{\"*\" * 50}\\n{red}Mean full{res} {metric_name} : {red}{mean_cv_score_full:.5f}{res}')\n",
    "    print(f'{red}Mean val{res} {metric_name}  : {red}{np.mean(val_scores):.5f}{res}')\n",
    "    \n",
    "    plot_training_process('auc', eval_results_, best_iters_, early_stopping_rounds)\n",
    "    plot_feature_importance(feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7e5de2",
   "metadata": {
    "papermill": {
     "duration": 0.184358,
     "end_time": "2023-06-29T23:23:43.692200",
     "exception": false,
     "start_time": "2023-06-29T23:23:43.507842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <p style=\"font-family:JetBrains Mono; font-weight:bold; letter-spacing: 2px; color:#1871c9; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #000966\">Weighted Ensemble Model by Optuna on Training</p>\n",
    "A weighted average is performed during training;  \n",
    "The weights were determined for each model using the predictions for the train data created in the out of fold with Optuna's CMAsampler. (Here it is defined by `OptunaWeights`)  \n",
    "This is an extension of the averaging method. All models are assigned different weights defining the importance of each model for prediction.\n",
    "\n",
    "![](https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Screen-Shot-2015-08-22-at-6.40.37-pm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4c19e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-29T23:23:44.056641Z",
     "iopub.status.busy": "2023-06-29T23:23:44.055959Z",
     "iopub.status.idle": "2023-06-29T23:23:44.071539Z",
     "shell.execute_reply": "2023-06-29T23:23:44.070358Z"
    },
    "papermill": {
     "duration": 0.201438,
     "end_time": "2023-06-29T23:23:44.073803",
     "exception": false,
     "start_time": "2023-06-29T23:23:43.872365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OptunaWeights:\n",
    "    def __init__(self, random_state, n_trials=100):\n",
    "        self.study = None\n",
    "        self.weights = None\n",
    "        self.random_state = random_state\n",
    "        self.n_trials = n_trials\n",
    "\n",
    "    def _objective(self, trial, y_true, y_preds):\n",
    "        # Define the weights for the predictions from each model\n",
    "        weights = [trial.suggest_float(f\"weight{n}\", 1e-15, 1) for n in range(len(y_preds))]\n",
    "\n",
    "        # Calculate the weighted prediction\n",
    "        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights)\n",
    "\n",
    "        # Calculate the score for the weighted prediction\n",
    "        score = metric(y_true, weighted_pred)\n",
    "        return score\n",
    "\n",
    "    def fit(self, y_true, y_preds):\n",
    "        optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n",
    "        pruner = optuna.pruners.HyperbandPruner()\n",
    "        self.study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=\"OptunaWeights\", direction='maximize')\n",
    "        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n",
    "        self.study.optimize(objective_partial, n_trials=self.n_trials)\n",
    "        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n",
    "\n",
    "    def predict(self, y_preds):\n",
    "        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n",
    "        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=self.weights)\n",
    "        return weighted_pred\n",
    "\n",
    "    def fit_predict(self, y_true, y_preds):\n",
    "        self.fit(y_true, y_preds)\n",
    "        return self.predict(y_preds)\n",
    "    \n",
    "    def weights(self):\n",
    "        return self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986561e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-29T23:23:44.440783Z",
     "iopub.status.busy": "2023-06-29T23:23:44.439679Z",
     "iopub.status.idle": "2023-06-30T01:45:43.900811Z",
     "shell.execute_reply": "2023-06-30T01:45:43.899556Z"
    },
    "papermill": {
     "duration": 8519.856954,
     "end_time": "2023-06-30T01:45:44.112504",
     "exception": false,
     "start_time": "2023-06-29T23:23:44.255550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test_predss_list = []\n",
    "oof_predss_list = []\n",
    "ensemble_score_list = []\n",
    "weights_list = []\n",
    "trained_models_list = []\n",
    "score_dict_list = []\n",
    "X_val_list = []\n",
    "oof_preds_list = []\n",
    "\n",
    "for target_col, X_train, y_train, X_test in zip(target_cols, X_trains, y_trains, X_tests):\n",
    "    print(f'=== {target_col} ===')\n",
    "    # drop_target_col = 'EC2' if target_col == 'EC1' else 'EC1'\n",
    "    # X_train = X_train.drop(drop_target_col, axis=1)\n",
    "\n",
    "    # Initialize an array for storing test predictions\n",
    "    classifier = Classifier(target_col, n_estimators, device, random_state)\n",
    "    test_predss = np.zeros((X_test.shape[0]))\n",
    "    oof_predss = np.zeros((X_train.shape[0], n_reapts))\n",
    "    ensemble_score, ensemble_score_ = [], []\n",
    "    weights = []\n",
    "    trained_models = dict(zip([_ for _ in classifier.models_name if ('xgb' in _) or ('lgb' in _) or ('cat' in _)], [[] for _ in range(classifier.len_models)]))\n",
    "    score_dict = dict(zip(classifier.models_name, [[] for _ in range(classifier.len_models)]))\n",
    "\n",
    "    splitter = Splitter(kfold=kfold, n_splits=n_splits, cat_df=y_train)\n",
    "    for i, (X_train_, X_val, y_train_, y_val, val_index) in enumerate(splitter.split_data(X_train, y_train, random_state_list=random_state_list)):\n",
    "        n = i % n_splits\n",
    "        m = i // n_splits\n",
    "\n",
    "        # Get a set of classifier models\n",
    "        classifier = Classifier(target_col, n_estimators, device, random_state_list[m])\n",
    "        models = classifier.models\n",
    "\n",
    "        # Initialize lists to store oof and test predictions for each base model\n",
    "        oof_preds = []\n",
    "        test_preds = []\n",
    "\n",
    "        # Loop over each base model and fit it to the training data, evaluate on validation data, and store predictions\n",
    "        for name, model in models.items():\n",
    "            best_iteration = None\n",
    "            start_time = time.time()\n",
    "\n",
    "            if ('xgb' in name) or ('lgb' in name) or ('cat' in name):\n",
    "                early_stopping_rounds_ = int(early_stopping_rounds*2) if ('lgb' in name) else early_stopping_rounds\n",
    "\n",
    "                if 'lgb' in name:\n",
    "                    model.fit(\n",
    "                        X_train_, y_train_, eval_set=[(X_val, y_val)], categorical_feature=cat_cols,\n",
    "                        early_stopping_rounds=early_stopping_rounds_, verbose=verbose)\n",
    "                elif 'cat' in name :\n",
    "                    model.fit(\n",
    "                        Pool(X_train_, y_train_, cat_features=cat_cols), eval_set=Pool(X_val, y_val, cat_features=cat_cols),\n",
    "                        early_stopping_rounds=early_stopping_rounds_, verbose=verbose)\n",
    "                else:\n",
    "                    model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)], early_stopping_rounds=early_stopping_rounds_, verbose=verbose)\n",
    "\n",
    "                best_iteration = model.best_iteration if ('xgb' in name) else model.best_iteration_\n",
    "            else:\n",
    "                model.fit(X_train_, y_train_)\n",
    "\n",
    "            end_time = time.time()\n",
    "            min_, sec = sec_to_minsec(end_time - start_time)\n",
    "\n",
    "            if name in trained_models.keys():\n",
    "                trained_models[f'{name}'].append(deepcopy(model))\n",
    "\n",
    "            y_val_pred = model.predict_proba(X_val)[:, 1].reshape(-1)\n",
    "            test_pred = model.predict_proba(X_test)[:, 1].reshape(-1)\n",
    "\n",
    "            score = metric(y_val, y_val_pred)\n",
    "            score_dict[name].append(score)\n",
    "            print(f'{blu}{name}{res} [FOLD-{n} SEED-{random_state_list[m]}] {metric_name} {blu}{score:.5f}{res} | Best iteration {blu}{best_iteration}{res} | Runtime {min_}min {sec}s')\n",
    "\n",
    "            oof_preds.append(y_val_pred)\n",
    "            test_preds.append(test_pred)\n",
    "\n",
    "        # Use Optuna to find the best ensemble weights\n",
    "        optweights = OptunaWeights(random_state=random_state_list[m], n_trials=n_trials)\n",
    "        y_val_pred = optweights.fit_predict(y_val.values, oof_preds)\n",
    "\n",
    "        score = metric(y_val, y_val_pred)\n",
    "        print(f'{red}>>> Ensemble{res} [FOLD-{n} SEED-{random_state_list[m]}] {metric_name} {red}{score:.5f}{res}')\n",
    "        print(f'{\"-\" * 60}')\n",
    "        ensemble_score.append(score)\n",
    "        weights.append(optweights.weights)\n",
    "\n",
    "        # Predict to X_test by the best ensemble weights\n",
    "        test_predss += optweights.predict(test_preds) / (n_splits * len(random_state_list))\n",
    "        oof_predss[X_val.index, m] += optweights.predict(oof_preds)\n",
    "\n",
    "        gc.collect()\n",
    "        \n",
    "    test_predss_list.append(test_predss)\n",
    "    oof_predss_list.append(oof_predss)\n",
    "    ensemble_score_list.append(ensemble_score)\n",
    "    weights_list.append(weights)\n",
    "    trained_models_list.append(trained_models)\n",
    "    score_dict_list.append(score_dict)\n",
    "    X_val_list.append(X_val)\n",
    "    oof_preds_list.append(oof_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa72021",
   "metadata": {
    "papermill": {
     "duration": 0.210973,
     "end_time": "2023-06-30T01:45:44.537330",
     "exception": false,
     "start_time": "2023-06-30T01:45:44.326357",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <p style=\"font-family:JetBrains Mono; font-weight:bold; letter-spacing: 2px; color:#1871c9; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #000966\">Mean Scores for each model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929470cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T01:45:44.963974Z",
     "iopub.status.busy": "2023-06-30T01:45:44.963101Z",
     "iopub.status.idle": "2023-06-30T01:45:45.841822Z",
     "shell.execute_reply": "2023-06-30T01:45:45.840733Z"
    },
    "papermill": {
     "duration": 1.093719,
     "end_time": "2023-06-30T01:45:45.844106",
     "exception": false,
     "start_time": "2023-06-30T01:45:44.750387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_score_from_dict(score_dict, title='', ascending=True):\n",
    "    score_df = pd.melt(pd.DataFrame(score_dict))\n",
    "    score_df = score_df.sort_values('value', ascending=ascending)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='value', y='variable', data=score_df, palette='Blues_r', errorbar='sd')\n",
    "    plt.xlabel(f'{title}', fontsize=14)\n",
    "    plt.ylabel('')\n",
    "    #plt.title(f'{title}', fontsize=18)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(True, axis='x')\n",
    "    plt.show()\n",
    "\n",
    "for target_col, score_dict in zip(target_cols, score_dict_list):\n",
    "    print(f'=== {target_col} ===') \n",
    "    print(f'--- Mean {metric_name} Scores---')    \n",
    "    for name, score in score_dict.items():\n",
    "        mean_score = np.mean(score)\n",
    "        std_score = np.std(score)\n",
    "        print(f'{name}: {red}{mean_score:.5f} ± {std_score:.5f}{res}')\n",
    "    plot_score_from_dict(score_dict, title=f'{metric_name} (n_splits:{n_splits})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067c8e05",
   "metadata": {
    "papermill": {
     "duration": 0.218477,
     "end_time": "2023-06-30T01:45:46.294695",
     "exception": false,
     "start_time": "2023-06-30T01:45:46.076218",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <p style=\"font-family:JetBrains Mono; font-weight:bold; letter-spacing: 2px; color:#1871c9; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #000966\">Weight of the Optuna Ensemble</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8208a7",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-06-30T01:45:46.724836Z",
     "iopub.status.busy": "2023-06-30T01:45:46.724004Z",
     "iopub.status.idle": "2023-06-30T01:45:49.631021Z",
     "shell.execute_reply": "2023-06-30T01:45:49.629945Z"
    },
    "papermill": {
     "duration": 3.124656,
     "end_time": "2023-06-30T01:45:49.633601",
     "exception": false,
     "start_time": "2023-06-30T01:45:46.508945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_heatmap_with_dendrogram(df, title, figsize=(18, 10), fontsize=10):\n",
    "    mask = np.zeros_like(df.astype(float).corr())\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    colormap = plt.cm.RdBu_r\n",
    "    fig, ax = plt.subplots(2, 1, figsize=figsize)\n",
    "\n",
    "    # Plot heatmap\n",
    "    ax[0].set_title(f'{title} Correlation of Features', fontweight='bold', y=1.02, size=15)\n",
    "    sns.heatmap(df.astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1.0,\n",
    "                square=True, cmap=colormap, linecolor='white', annot=True,\n",
    "                annot_kws={\"size\": fontsize, \"weight\": \"bold\"}, mask=mask, ax=ax[0], cbar=False)\n",
    "\n",
    "    # Plot dendrogram\n",
    "    correlations = df.corr()\n",
    "    converted_corr = 1 - np.abs(correlations)\n",
    "    Z = linkage(squareform(converted_corr), 'complete')\n",
    "    \n",
    "    dn = dendrogram(Z, labels=df.columns, above_threshold_color='#ff0000', ax=ax[1])\n",
    "    ax[1].set_title(f'{title} Hierarchical Clustering Dendrogram', fontsize=15, fontweight='bold')\n",
    "    ax[1].grid(axis='x')\n",
    "    ax[1].tick_params(axis='x', rotation=90)\n",
    "    ax[1].tick_params(axis='y', labelsize=fontsize)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for target_col, ensemble_score, weights, oof_preds, score_dict in zip(target_cols, ensemble_score_list, weights_list, oof_preds_list, score_dict_list):\n",
    "    print(f'=== {target_col} ===') \n",
    "\n",
    "    # Calculate the mean LogLoss score of the ensemble\n",
    "    mean_score = np.mean(ensemble_score)\n",
    "    std_score = np.std(ensemble_score)\n",
    "    print(f'{red}Mean{res} Optuna Ensemble {metric_name} {red}{mean_score:.5f} ± {std_score:.5f}{res}')\n",
    "\n",
    "    print('')\n",
    "    # Print the mean and standard deviation of the ensemble weights for each model\n",
    "    print('--- Optuna Weights---')\n",
    "    mean_weights = np.mean(weights, axis=0)\n",
    "    std_weights = np.std(weights, axis=0)\n",
    "    for name, mean_weight, std_weight in zip(models.keys(), mean_weights, std_weights):\n",
    "        print(f'{name}: {blu}{mean_weight:.5f} ± {std_weight:.5f}{res}')\n",
    "\n",
    "    # Plot Optuna Weights\n",
    "    normalize = [((weight - np.min(weight)) / (np.max(weight) - np.min(weight))).tolist() for weight in weights]\n",
    "    weight_dict = dict(zip(list(score_dict.keys()), np.array(normalize).T.tolist()))\n",
    "    plot_score_from_dict(weight_dict, title='Optuna Weights (Normalize 0 to 1)', ascending=False)\n",
    "    \n",
    "    # Plot oof_predict analyis for each model\n",
    "    plot_heatmap_with_dendrogram(pd.DataFrame(oof_preds, index=list(score_dict.keys())).T, title='OOF Predict', figsize=(10, 10), fontsize=8)\n",
    "\n",
    "mean_score = np.mean(ensemble_score_list)\n",
    "std_score = np.std(ensemble_score_list)\n",
    "print(f'{red} EC1 and EC2 Mean{res} Optuna Ensemble {metric_name} {red}{mean_score:.5f} ± {std_score:.5f}{res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0750b17d",
   "metadata": {
    "papermill": {
     "duration": 0.216186,
     "end_time": "2023-06-30T01:45:50.083269",
     "exception": false,
     "start_time": "2023-06-30T01:45:49.867083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <p style=\"font-family:JetBrains Mono; font-weight:bold; letter-spacing: 2px; color:#1871c9; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #000966\">SHAP Analysis</p>\n",
    "SHAP stands for SHapley Additive exPlanations, a method for determining the contribution of each variable (feature) to the model's predicted outcome. Since SHAP cannot be adapted for ensemble models, let's use SHAP to understand `Xgboost` and `Catboost`.\n",
    "\n",
    "**Consideration of Results:**  \n",
    "\n",
    "![](https://data-analysis-stats.jp/wp-content/uploads/2020/01/SHAP02.png)\n",
    "\n",
    "Reference1. https://meichenlu.com/2018-11-10-SHAP-explainable-machine-learning/  \n",
    "Reference2. https://christophm.github.io/interpretable-ml-book/shap.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eec318e",
   "metadata": {
    "papermill": {
     "duration": 0.215519,
     "end_time": "2023-06-30T01:45:50.517078",
     "exception": false,
     "start_time": "2023-06-30T01:45:50.301559",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <p style=\"font-family:JetBrains Mono; font-weight:bold; letter-spacing: 2px; color:#1871c9; font-size:85%; text-align:left;padding: 0px; border-bottom: 3px solid #000966\">Xgboost EC1</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd71b521",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T01:45:50.952988Z",
     "iopub.status.busy": "2023-06-30T01:45:50.952352Z",
     "iopub.status.idle": "2023-06-30T01:46:18.557339Z",
     "shell.execute_reply": "2023-06-30T01:46:18.556023Z"
    },
    "papermill": {
     "duration": 27.826718,
     "end_time": "2023-06-30T01:46:18.561408",
     "exception": false,
     "start_time": "2023-06-30T01:45:50.734690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(model=trained_models_list[0]['xgb'][-1])\n",
    "shap_values = explainer.shap_values(X=X_val_list[0])\n",
    "\n",
    "# Bar plot\n",
    "plt.figure(figsize=(20, 14))\n",
    "shap.summary_plot(shap_values, X_val_list[0], plot_type=\"bar\", show=False)\n",
    "plt.title(\"Feature Importance - Bar\", fontsize=16)\n",
    "\n",
    "# Dot plot\n",
    "plt.figure(figsize=(20, 14))\n",
    "shap.summary_plot(shap_values, X_val_list[0], plot_type=\"dot\", show=False)\n",
    "plt.title(\"Feature Importance - Dot\", fontsize=16)\n",
    "\n",
    "# Adjust layout and display the plots side by side\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f670fdd",
   "metadata": {
    "papermill": {
     "duration": 0.237911,
     "end_time": "2023-06-30T01:46:19.048442",
     "exception": false,
     "start_time": "2023-06-30T01:46:18.810531",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <p style=\"font-family:JetBrains Mono; font-weight:bold; letter-spacing: 2px; color:#1871c9; font-size:85%; text-align:left;padding: 0px; border-bottom: 3px solid #000966\">Catboost EC1</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e009b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T01:46:19.508874Z",
     "iopub.status.busy": "2023-06-30T01:46:19.508219Z",
     "iopub.status.idle": "2023-06-30T01:46:24.460160Z",
     "shell.execute_reply": "2023-06-30T01:46:24.459311Z"
    },
    "papermill": {
     "duration": 5.185998,
     "end_time": "2023-06-30T01:46:24.463455",
     "exception": false,
     "start_time": "2023-06-30T01:46:19.277457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(model=trained_models_list[0]['cat'][-1])\n",
    "shap_values = explainer.shap_values(X=X_val_list[0])\n",
    "\n",
    "# Bar plot\n",
    "plt.figure(figsize=(20, 14))\n",
    "shap.summary_plot(shap_values, X_val_list[0], plot_type=\"bar\", show=False)\n",
    "plt.title(\"Feature Importance - Bar\", fontsize=16)\n",
    "\n",
    "# Dot plot\n",
    "plt.figure(figsize=(20, 14))\n",
    "shap.summary_plot(shap_values, X_val_list[0], plot_type=\"dot\", show=False)\n",
    "plt.title(\"Feature Importance - Dot\", fontsize=16)\n",
    "\n",
    "# Adjust layout and display the plots side by side\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c177e4",
   "metadata": {
    "papermill": {
     "duration": 0.239677,
     "end_time": "2023-06-30T01:46:24.956931",
     "exception": false,
     "start_time": "2023-06-30T01:46:24.717254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <p style=\"font-family:JetBrains Mono; font-weight:bold; letter-spacing: 2px; color:#1871c9; font-size:85%; text-align:left;padding: 0px; border-bottom: 3px solid #000966\">Xgboost EC2</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a69b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T01:46:25.438331Z",
     "iopub.status.busy": "2023-06-30T01:46:25.437335Z",
     "iopub.status.idle": "2023-06-30T01:46:29.148540Z",
     "shell.execute_reply": "2023-06-30T01:46:29.147676Z"
    },
    "papermill": {
     "duration": 3.95649,
     "end_time": "2023-06-30T01:46:29.152584",
     "exception": false,
     "start_time": "2023-06-30T01:46:25.196094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(model=trained_models_list[1]['xgb'][-1])\n",
    "shap_values = explainer.shap_values(X=X_val_list[1])\n",
    "\n",
    "# Bar plot\n",
    "plt.figure(figsize=(20, 14))\n",
    "shap.summary_plot(shap_values, X_val_list[1], plot_type=\"bar\", show=False)\n",
    "plt.title(\"Feature Importance - Bar\", fontsize=16)\n",
    "\n",
    "# Dot plot\n",
    "plt.figure(figsize=(20, 14))\n",
    "shap.summary_plot(shap_values, X_val_list[1], plot_type=\"dot\", show=False)\n",
    "plt.title(\"Feature Importance - Dot\", fontsize=16)\n",
    "\n",
    "# Adjust layout and display the plots side by side\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05744730",
   "metadata": {
    "papermill": {
     "duration": 0.269286,
     "end_time": "2023-06-30T01:46:29.697410",
     "exception": false,
     "start_time": "2023-06-30T01:46:29.428124",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <p style=\"font-family:JetBrains Mono; font-weight:bold; letter-spacing: 2px; color:#1871c9; font-size:85%; text-align:left;padding: 0px; border-bottom: 3px solid #000966\">Catboost EC2</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745afe83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T01:46:30.201416Z",
     "iopub.status.busy": "2023-06-30T01:46:30.200208Z",
     "iopub.status.idle": "2023-06-30T01:46:38.017996Z",
     "shell.execute_reply": "2023-06-30T01:46:38.016661Z"
    },
    "papermill": {
     "duration": 8.073195,
     "end_time": "2023-06-30T01:46:38.021731",
     "exception": false,
     "start_time": "2023-06-30T01:46:29.948536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(model=trained_models_list[1]['cat'][-1])\n",
    "shap_values = explainer.shap_values(X=X_val_list[1])\n",
    "\n",
    "# Bar plot\n",
    "plt.figure(figsize=(20, 14))\n",
    "shap.summary_plot(shap_values, X_val_list[1], plot_type=\"bar\", show=False)\n",
    "plt.title(\"Feature Importance - Bar\", fontsize=16)\n",
    "\n",
    "# Dot plot\n",
    "plt.figure(figsize=(20, 14))\n",
    "shap.summary_plot(shap_values, X_val_list[1], plot_type=\"dot\", show=False)\n",
    "plt.title(\"Feature Importance - Dot\", fontsize=16)\n",
    "\n",
    "# Adjust layout and display the plots side by side\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fe98b8",
   "metadata": {
    "papermill": {
     "duration": 0.265339,
     "end_time": "2023-06-30T01:46:38.573273",
     "exception": false,
     "start_time": "2023-06-30T01:46:38.307934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <p style=\"font-family:JetBrains Mono; font-weight:bold; letter-spacing: 2px; color:#1871c9; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #000966\">Make Submission</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bb1c92",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-06-30T01:46:39.095295Z",
     "iopub.status.busy": "2023-06-30T01:46:39.094639Z",
     "iopub.status.idle": "2023-06-30T01:46:40.805882Z",
     "shell.execute_reply": "2023-06-30T01:46:40.804727Z"
    },
    "papermill": {
     "duration": 1.975964,
     "end_time": "2023-06-30T01:46:40.808191",
     "exception": false,
     "start_time": "2023-06-30T01:46:38.832227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def show_confusion_roc(oof, title='Model Evaluation Results'):\n",
    "    f, ax = plt.subplots(1, 2, figsize=(13.3, 4))\n",
    "    df = pd.DataFrame(np.stack([oof[0], oof[1]]), index=['preds', 'target']).T\n",
    "    cm = confusion_matrix(df.target, df.preds.ge(0.5).astype(int))\n",
    "    cm_display = ConfusionMatrixDisplay(cm).plot(cmap='GnBu_r', ax=ax[0])\n",
    "    ax[0].grid(False)\n",
    "    RocCurveDisplay.from_predictions(df.target, df.preds, ax=ax[1])\n",
    "    ax[1].grid(True)\n",
    "    plt.suptitle(f'{title}', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    #plt.grid()\n",
    "\n",
    "for target_col, y_train, oof_predss in zip(target_cols, y_trains, oof_predss_list):\n",
    "    show_confusion_roc(oof=[np.mean(oof_predss, axis=1), y_train], title=f'{target_col} OOF Evaluation Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e383c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T01:46:41.368054Z",
     "iopub.status.busy": "2023-06-30T01:46:41.367520Z",
     "iopub.status.idle": "2023-06-30T01:46:41.460205Z",
     "shell.execute_reply": "2023-06-30T01:46:41.458956Z"
    },
    "papermill": {
     "duration": 0.366143,
     "end_time": "2023-06-30T01:46:41.462668",
     "exception": false,
     "start_time": "2023-06-30T01:46:41.096525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_submission(target_cols, test_predss_list, prefix=''):\n",
    "    sub = pd.read_csv(os.path.join(filepath, 'sample_submission.csv'))\n",
    "    for target_col, test_predss in zip(target_cols, test_predss_list):\n",
    "        sub[f'{target_col}'] = test_predss\n",
    "    sub.to_csv(f'{prefix}submission.csv', index=False)\n",
    "    return  sub\n",
    "\n",
    "sub = make_submission(target_cols, test_predss_list, prefix='')\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7743b4",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-06-30T01:46:41.994221Z",
     "iopub.status.busy": "2023-06-30T01:46:41.993555Z",
     "iopub.status.idle": "2023-06-30T01:46:43.143643Z",
     "shell.execute_reply": "2023-06-30T01:46:43.142489Z"
    },
    "papermill": {
     "duration": 1.420416,
     "end_time": "2023-06-30T01:46:43.146101",
     "exception": false,
     "start_time": "2023-06-30T01:46:41.725685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_distribution(filepath, sub, target_col):\n",
    "    # df_train = pd.read_csv(os.path.join(filepath, 'train.csv'), index_col=[0])\n",
    "    # df_test = pd.read_csv(os.path.join(filepath, 'test.csv'), index_col=[0])\n",
    "    # original = pd.read_csv('/kaggle/input/machine-failure-predictions/machine failure.csv', index_col=[0])\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    sns.kdeplot(data=sub, x=target_col, fill=True, alpha=0.5, common_norm=False, label=f\"{target_col} Predict\")\n",
    "    # sns.kdeplot(data=df_train, x=target_col, fill=True, alpha=0.5, common_norm=False, label=\"Data\")\n",
    "    # sns.kdeplot(data=original, x=target_col, fill=True, alpha=0.5, common_norm=False, label=\"Original\")\n",
    "\n",
    "    plt.title('Predictive vs Training Distribution')\n",
    "    plt.legend()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "    \n",
    "for target_col in target_cols:\n",
    "    plot_distribution(filepath, sub, target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a594a95",
   "metadata": {
    "papermill": {
     "duration": 0.267869,
     "end_time": "2023-06-30T01:46:43.706860",
     "exception": false,
     "start_time": "2023-06-30T01:46:43.438991",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <p style=\"font-family:JetBrains Mono; font-weight:bold; letter-spacing: 2px; color:#1871c9; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #000966\">Meta Prediction</p>\n",
    "Model based on EC1 and EC2 forecast results in the supplement.\n",
    "\n",
    "**Update: CV scores increased, but LB scores decreased. It may be that they are overlearning.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21106e89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T01:46:44.239549Z",
     "iopub.status.busy": "2023-06-30T01:46:44.238401Z",
     "iopub.status.idle": "2023-06-30T01:46:44.253161Z",
     "shell.execute_reply": "2023-06-30T01:46:44.252138Z"
    },
    "papermill": {
     "duration": 0.284546,
     "end_time": "2023-06-30T01:46:44.255852",
     "exception": false,
     "start_time": "2023-06-30T01:46:43.971306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# test_predss_list = []\n",
    "# # oof_predss_list = []\n",
    "# ensemble_score_list = []\n",
    "# weights_list = []\n",
    "# trained_models_list = []\n",
    "# score_dict_list = []\n",
    "# X_val_list = []\n",
    "\n",
    "# for target_col, X_train, y_train, X_test, oof_predss in zip(target_cols, X_trains, y_trains, X_tests, oof_predss_list):\n",
    "#     print(f'=== {target_col} ===')\n",
    "#     drop_target_col = 'EC2' if target_col == 'EC1' else 'EC1'\n",
    "    \n",
    "#     # X_train[drop_target_col] = np.mean(oof_predss, axis=1)\n",
    "#     X_test = pd.concat([X_test, sub[drop_target_col]], axis=1)\n",
    "\n",
    "#     # Initialize an array for storing test predictions\n",
    "#     n_estimators = 300\n",
    "#     classifier = Classifier(target_col, n_estimators, device, random_state)\n",
    "#     test_predss = np.zeros((X_test.shape[0]))\n",
    "#     oof_predss = np.zeros((X_train.shape[0], n_reapts))\n",
    "#     ensemble_score, ensemble_score_ = [], []\n",
    "#     weights = []\n",
    "#     trained_models = dict(zip([_ for _ in classifier.models_name if ('xgb' in _) or ('lgb' in _) or ('cat' in _)], [[] for _ in range(classifier.len_models)]))\n",
    "#     score_dict = dict(zip(classifier.models_name, [[] for _ in range(classifier.len_models)]))\n",
    "\n",
    "#     splitter = Splitter(kfold=kfold, n_splits=n_splits, cat_df=y_train)\n",
    "#     for i, (X_train_, X_val, y_train_, y_val, val_index) in enumerate(splitter.split_data(X_train, y_train, random_state_list=random_state_list)):\n",
    "#         n = i % n_splits\n",
    "#         m = i // n_splits\n",
    "\n",
    "#         # Get a set of classifier models\n",
    "#         classifier = Classifier(target_col, n_estimators, device, random_state_list[m])\n",
    "#         models = classifier.models\n",
    "\n",
    "#         # Initialize lists to store oof and test predictions for each base model\n",
    "#         oof_preds = []\n",
    "#         test_preds = []\n",
    "\n",
    "#         # Loop over each base model and fit it to the training data, evaluate on validation data, and store predictions\n",
    "#         for name, model in models.items():\n",
    "#             best_iteration = None\n",
    "#             start_time = time.time()\n",
    "\n",
    "#             if ('xgb' in name) or ('lgb' in name) or ('cat' in name):\n",
    "#                 early_stopping_rounds_ = int(early_stopping_rounds*2) if ('xgb' not in name) else early_stopping_rounds\n",
    "\n",
    "#                 if 'lgb' in name:\n",
    "#                     model.fit(\n",
    "#                         X_train_, y_train_, eval_set=[(X_val, y_val)], categorical_feature=cat_cols,\n",
    "#                         early_stopping_rounds=early_stopping_rounds_, verbose=verbose)\n",
    "#                 elif 'cat' in name :\n",
    "#                     model.fit(\n",
    "#                         Pool(X_train_, y_train_, cat_features=cat_cols), eval_set=Pool(X_val, y_val, cat_features=cat_cols),\n",
    "#                         early_stopping_rounds=early_stopping_rounds_, verbose=verbose)\n",
    "#                 else:\n",
    "#                     model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)], early_stopping_rounds=early_stopping_rounds_, verbose=verbose)\n",
    "\n",
    "#                 best_iteration = model.best_iteration if ('xgb' in name) else model.best_iteration_\n",
    "#             else:\n",
    "#                 model.fit(X_train_, y_train_)\n",
    "\n",
    "#             end_time = time.time()\n",
    "#             min_, sec = sec_to_minsec(end_time - start_time)\n",
    "\n",
    "#             if name in trained_models.keys():\n",
    "#                 trained_models[f'{name}'].append(deepcopy(model))\n",
    "\n",
    "#             y_val_pred = model.predict_proba(X_val)[:, 1].reshape(-1)\n",
    "#             test_pred = model.predict_proba(X_test)[:, 1].reshape(-1)\n",
    "\n",
    "#             score = metric(y_val, y_val_pred)\n",
    "#             score_dict[name].append(score)\n",
    "#             print(f'{blu}{name}{res} [FOLD-{n} SEED-{random_state_list[m]}] {metric_name} {blu}{score:.5f}{res} | Best iteration {blu}{best_iteration}{res} | Runtime {min_}min {sec}s')\n",
    "\n",
    "#             oof_preds.append(y_val_pred)\n",
    "#             test_preds.append(test_pred)\n",
    "\n",
    "#         # Use Optuna to find the best ensemble weights\n",
    "#         optweights = OptunaWeights(random_state=random_state_list[m], n_trials=n_trials)\n",
    "#         y_val_pred = optweights.fit_predict(y_val.values, oof_preds)\n",
    "\n",
    "#         score = metric(y_val, y_val_pred)\n",
    "#         print(f'{red}>>> Ensemble{res} [FOLD-{n} SEED-{random_state_list[m]}] {metric_name} {red}{score:.5f}{res}')\n",
    "#         print(f'{\"-\" * 60}')\n",
    "#         ensemble_score.append(score)\n",
    "#         weights.append(optweights.weights)\n",
    "\n",
    "#         # Predict to X_test by the best ensemble weights\n",
    "#         test_predss += optweights.predict(test_preds) / (n_splits * len(random_state_list))\n",
    "#         oof_predss[X_val.index, m] += optweights.predict(oof_preds)\n",
    "\n",
    "#         gc.collect()\n",
    "        \n",
    "#     test_predss_list.append(test_predss)\n",
    "#     oof_predss_list.append(oof_predss)\n",
    "#     ensemble_score_list.append(ensemble_score)\n",
    "#     weights_list.append(weights)\n",
    "#     trained_models_list.append(trained_models)\n",
    "#     score_dict_list.append(score_dict)\n",
    "#     X_val_list.append(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a698b339",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-06-30T01:46:44.792627Z",
     "iopub.status.busy": "2023-06-30T01:46:44.791421Z",
     "iopub.status.idle": "2023-06-30T01:46:44.798863Z",
     "shell.execute_reply": "2023-06-30T01:46:44.797473Z"
    },
    "papermill": {
     "duration": 0.278583,
     "end_time": "2023-06-30T01:46:44.801425",
     "exception": false,
     "start_time": "2023-06-30T01:46:44.522842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for target_col, ensemble_score in zip(target_cols, ensemble_score_list):\n",
    "#     print(f'=== {target_col} ===') \n",
    "\n",
    "#     # Calculate the mean LogLoss score of the ensemble\n",
    "#     mean_score = np.mean(ensemble_score)\n",
    "#     std_score = np.std(ensemble_score)\n",
    "#     print(f'{red}Mean{res} Optuna Ensemble {metric_name} {red}{mean_score:.5f} ± {std_score:.5f}{res}')\n",
    "\n",
    "# print('')\n",
    "# mean_score = np.mean(ensemble_score_list)\n",
    "# std_score = np.std(ensemble_score_list)\n",
    "# print(f'{red} EC1 and EC2 Mean{res} Optuna Ensemble {metric_name} {red}{mean_score:.5f} ± {std_score:.5f}{res}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a684fdd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T01:46:45.338954Z",
     "iopub.status.busy": "2023-06-30T01:46:45.337742Z",
     "iopub.status.idle": "2023-06-30T01:46:45.342121Z",
     "shell.execute_reply": "2023-06-30T01:46:45.341336Z"
    },
    "papermill": {
     "duration": 0.277418,
     "end_time": "2023-06-30T01:46:45.345332",
     "exception": false,
     "start_time": "2023-06-30T01:46:45.067914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sub = make_submission(target_cols, test_predss_list, prefix='meta_')\n",
    "# sub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9651.607754,
   "end_time": "2023-06-30T01:46:48.463062",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-29T23:05:56.855308",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
